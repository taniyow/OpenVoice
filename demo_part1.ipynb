{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ee1ede",
   "metadata": {},
   "source": [
    "## Voice Style Control Demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7f043ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from openvoice import se_extractor\n",
    "from openvoice.api import BaseSpeakerTTS, ToneColorConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15116b59",
   "metadata": {},
   "source": [
    "### Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aacad912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "d:\\Dev\\ariolas-tech\\rnd\\OpenVoice\\openvoice\\api.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(ckpt_path, map_location=torch.device(self.device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/base_speakers/EN/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\wavmark\\__init__.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(resume_path, map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n"
     ]
    }
   ],
   "source": [
    "ckpt_base = \"checkpoints/base_speakers/EN\"\n",
    "ckpt_converter = \"checkpoints/converter\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "output_dir = \"outputs\"\n",
    "\n",
    "base_speaker_tts = BaseSpeakerTTS(f\"{ckpt_base}/config.json\", device=device)\n",
    "base_speaker_tts.load_ckpt(f\"{ckpt_base}/checkpoint.pth\")\n",
    "\n",
    "tone_color_converter = ToneColorConverter(\n",
    "    f\"{ckpt_converter}/config.json\", device=device\n",
    ")\n",
    "tone_color_converter.load_ckpt(f\"{ckpt_converter}/checkpoint.pth\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67740c",
   "metadata": {},
   "source": [
    "### Obtain Tone Color Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8add279",
   "metadata": {},
   "source": [
    "The `source_se` is the tone color embedding of the base speaker.\n",
    "It is an average of multiple sentences generated by the base speaker. We directly provide the result here but\n",
    "the readers feel free to extract `source_se` by themselves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63ff6273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taniyow\\AppData\\Local\\Temp\\ipykernel_12404\\873191364.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  source_se = torch.load(f\"{ckpt_base}/en_default_se.pth\").to(device)\n"
     ]
    }
   ],
   "source": [
    "source_se = torch.load(f\"{ckpt_base}/en_default_se.pth\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71fcc3",
   "metadata": {},
   "source": [
    "The `reference_speaker.mp3` below points to the short audio clip of the reference whose voice we want to clone. We provide an example here. If you use your own reference speakers, please **make sure each speaker has a unique filename.** The `se_extractor` will save the `targeted_se` using the filename of the audio and **will not automatically overwrite.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55105eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning voice from resources/5 wild new AI tools you can try right now - Fireship (youtube).mp3\n",
      "d:\\Dev\\ariolas-tech\\rnd\\OpenVoice\\resources\\example_reference.mp3\n",
      "Current working directory: d:\\Dev\\ariolas-tech\\rnd\\OpenVoice\n"
     ]
    }
   ],
   "source": [
    "reference_speaker = \"resources/5 wild new AI tools you can try right now - Fireship (youtube).mp3\"  # This is the voice you want to clone\n",
    "print(f\"Cloning voice from {reference_speaker}\")\n",
    "print(os.path.abspath(\"resources/example_reference.mp3\"))\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff0a93f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: whisper in c:\\users\\taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages (1.1.10)\n",
      "Requirement already satisfied: six in c:\\users\\taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages (from whisper) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install whisper\n",
    "# ffmpeg should be installed in your system as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d915b782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v1\n",
      "[(0.0, 165.33), (166.19, 168.21), (169.422, 254.304)]\n",
      "after vad: dur = 252.232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\functional.py:666: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\SpectralOps.cpp:878.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "target_se, audio_name = se_extractor.get_se(\n",
    "    reference_speaker, tone_color_converter, target_dir=\"processed\", vad=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40284aa",
   "metadata": {},
   "source": [
    "### Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32f206fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text resource:  \n",
      "    This audio is generated by OpenVoice.\n",
      "    It features voice style control such as friendly, cheerful,\n",
      "    sad, angry, terrified, shouting, whispering, or default speakers.\n",
      "    The voice speed can also be adjusted.\n",
      "    OpenAI text-to-speech can be used as base speaker to generate the audio for different languages.\n",
      "    \n",
      " > Text splitted to sentences.\n",
      "This audio is generated by OpenVoice. It features voice style control such as friendly,\n",
      "cheerful, sad, angry, terrified, shouting, whispering, or default speakers. The voice speed can also be adjusted.\n",
      "OpenAI text-to-speech can be used as base speaker to generate the audio for different languages.\n",
      " > ===========================\n",
      "ðɪs ˈɑdiˌoʊ ɪz ˈdʒɛnəɹˌeɪtɪd baɪ ˈoʊpən vɔɪs. ɪt ˈfitʃəɹz vɔɪs staɪɫ kənˈtɹoʊɫ sətʃ ɛz ˈfɹɛndli,\n",
      " length:96\n",
      " length:96\n",
      "ˈtʃɪɹfəɫ, sæd, ˈæŋgɹi, ˈtɛɹəˌfaɪd, ˈʃaʊtɪŋ, ˈwɪspəɹɪŋ, əɹ dɪˈfɔɫt ˈspikəɹz. ðə vɔɪs spid kən ˈɔlsoʊ bi əˈdʒəstɪd.\n",
      " length:113\n",
      " length:113\n",
      "ˈoʊpən eɪaɪ text-to-speech* kən bi juzd ɛz beɪs ˈspikəɹ tɪ ˈdʒɛnəɹˌeɪt ðə ˈɑdiˌoʊ fəɹ ˈdɪfəɹənt ˈlæŋgwɪdʒɪz.\n",
      " length:108\n",
      " length:107\n"
     ]
    }
   ],
   "source": [
    "save_path = f\"{output_dir}/output_en_sample.wav\"\n",
    "\n",
    "# Run the base speaker tts\n",
    "text = \"\"\"\n",
    "    This audio is generated by OpenVoice.\n",
    "    It features voice style control such as friendly, cheerful,\n",
    "    sad, angry, terrified, shouting, whispering, or default speakers.\n",
    "    The voice speed can also be adjusted.\n",
    "    OpenAI text-to-speech can be used as base speaker to generate the audio for different languages.\n",
    "    \"\"\"\n",
    "# text = \"\"\"\n",
    "#     Hello, I am Claude, your personal assistant. My audio is cloned as reference and generated this audio using OpenVoice V1.\n",
    "#     A custom text can be passed to generate the audio in the desired tone.\n",
    "#     \"\"\"\n",
    "\n",
    "print(\"Text resource: \", text)\n",
    "\n",
    "src_path = f\"{output_dir}/tmp.wav\"\n",
    "base_speaker_tts.tts(text, src_path, speaker=\"default\", language=\"English\", speed=1.0)\n",
    "\n",
    "# Run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path,\n",
    "    src_se=source_se,\n",
    "    tgt_se=target_se,\n",
    "    output_path=save_path,\n",
    "    message=encode_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73dc1259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text resource:  One year ago, this unbelievable video of Will Smith eating spaghetti took the world by storm. We humans joked about it. We could easily tell that it was fake, and at that point, nobody was really afraid. But fast forward one year later, and generative AI tech has taken another huge leap forward. Will Smith eating spaghetti in 2024 is nothing to joke around about. If it doesn't plateau soon, it could put our Hollywood idols out of business, and there would be no one left to brainwash us. In today's video, we'll descend further into Uncanny Valley, and look at five new generative AI tools that you can actually use today. By the end of this video, you'll be able to fire your human photographer, videographer, sound engineer, and programmer. It is June 17th, 2024, and you're watching The Code Report. A few months ago, OpenAI previewed Sora, and teased us with a bunch of AI videos. Google later followed that up with Veeo, which was also quite impressive, but just this week, the Chinese dropped a new model called Kling. They can generate videos two minutes long, up to 30 FPS. It's incredibly impressive, and arguably better than Sora. But there's one big problem with all of these models. They're not available to the public. Well luckily, a new tool just dropped called the Dream Machine from LumaLabs, and it allows you to create relatively realistic video clips. I prompted it for two old men doing yoga, and the result is indistinguishable from real life, unless of course you look very closely at the fingers. In addition, this is the tool used to generate a realistic Will Smith eating spaghetti, and while impressive, there's really no practical or commercial use for this tool yet. All it's really good at is simulating nightmares. Before we move on to the next topic though, let's talk about something your LLMs and AI models can't live without, data. Data collection on the web used to be a nightmare. You'd have to set up proxy networks, develop web unblockers to bypass CAPTCHAs, deal with server errors, browser fingerprinting, and all sorts of other issues. But with residential proxies and web automation tools like Selenium, Puppeteer, and Playwright, you can scrape the web on a massive scale, without blowing up your budget in the process. That's where Bright Data comes in, the sponsor of today's video. They take your scrape ops to the next level at a fraction of the cost. With their Scraping Browser API, you can forget about proxies and web unblockers. Everything you need to scrape data at scale is under the hood, making your web scrapers unstoppable. Try Bright Data's Scraping Browser API for free right now with the link in the description. But the next big AI tool you need to know about is Stable Diffusion 3 Medium. The model weights were just released hours ago, and it's the most advanced open text to image model out there. Unfortunately, it's only available under a non-commercial license, but the level of quality is pretty amazing, and can now reliably generate text from your prompts. Now if you currently have an AI girlfriend, I would highly recommend upgrading her to this new model. It's the personality that matters, but she's going to be looking pretty mid compared to all your friends on ST3. Now another tool that's actually useful is this sound effect generator from Eleven Labs, the same company that engineered my voice. All you have to do is describe what you want to hear, and it will generate multiple sound effects. Here's an example of two different results. And example two. Now what I failed to tell you is that one example was real, and one was AI generated. I bet you can't smell the difference, but now we need to talk about code generation. I've been patiently waiting for AI to take my programming job, but so far I've been disappointed. However, there may still be hope. A few weeks ago, the French startup Mistral released a new model called CodeStroll. It's also an open model, but cannot be used for commercial purposes yet, and performs extremely well on coding benchmarks compared to other open models. I've been playing around with it on Ollama, and while it's extremely impressive, and also very fast, it still struggles with the age old problem of centering a div. Now when it comes to AI writing code, there are two types of people. There's people like Devin who are doing AI maxing, and trying to get AI to write nearly 100% of our code. These people are usually young and naive, according to the AI doomers on the other end of the spectrum. They're usually boomers who think that AI code is total slop, and has no place in the industry. But the optimal hot take is likely somewhere in between. Another free tool you can use right now is Cursor, which is a fork of VS Code, and one of the first truly AI focused code editors. Instead of memorizing syntax, give it the context of an existing code base or documentation, then hit Ctrl-K and write your code with natural language. And if you're worried that it's writing garbage code, you can enforce certain rules and even have it perform a code review. It's like GitHub Copilot on steroids. Generative AI still has a long way to go, but if I were Jada right now, I'd be very concerned about the amount of progress made by these data science nerds in just the last year. This has been the Code Report. Thanks for watching, and I will see you in the next one.\n",
      "\n",
      " > Text splitted to sentences.\n",
      "One year ago, this unbelievable video of Will Smith eating spaghetti took the world by storm.\n",
      "We humans joked about it. We could easily tell that it was fake,\n",
      "and at that point, nobody was really afraid. But fast forward one year later,\n",
      "and generative AI tech has taken another huge leap forward. Will Smith eating spaghetti in 2024 is nothing to joke around about.\n",
      "If it doesn't plateau soon, it could put our Hollywood idols out of business,\n",
      "and there would be no one left to brainwash us. In today's video,\n",
      "we'll descend further into Uncanny Valley, and look at five new generative AI tools that you can actually use today.\n",
      "By the end of this video, you'll be able to fire your human photographer,\n",
      "videographer, sound engineer, and programmer. It is June 17th, 2024, and you're watching The Code Report.\n",
      "A few months ago, OpenAI previewed Sora, and teased us with a bunch of AI videos.\n",
      "Google later followed that up with Veeo, which was also quite impressive,\n",
      "but just this week, the Chinese dropped a new model called Kling.\n",
      "They can generate videos two minutes long, up to 30 FPS.\n",
      "It's incredibly impressive, and arguably better than Sora. But there's one big problem with all of these models.\n",
      "They're not available to the public. Well luckily, a new tool just dropped called the Dream Machine from LumaLabs,\n",
      "and it allows you to create relatively realistic video clips. I prompted it for two old men doing yoga,\n",
      "and the result is indistinguishable from real life, unless of course you look very closely at the fingers.\n",
      "In addition, this is the tool used to generate a realistic Will Smith eating spaghetti,\n",
      "and while impressive, there's really no practical or commercial use for this tool yet.\n",
      "All it's really good at is simulating nightmares. Before we move on to the next topic though,\n",
      "let's talk about something your LLMs and AI models can't live without,\n",
      "data. Data collection on the web used to be a nightmare.\n",
      "You'd have to set up proxy networks, develop web unblockers to bypass CAPTCHAs,\n",
      "deal with server errors, browser fingerprinting, and all sorts of other issues.\n",
      "But with residential proxies and web automation tools like Selenium, Puppeteer,\n",
      "and Playwright, you can scrape the web on a massive scale,\n",
      "without blowing up your budget in the process. That's where Bright Data comes in,\n",
      "the sponsor of today's video. They take your scrape ops to the next level at a fraction of the cost.\n",
      "With their Scraping Browser API, you can forget about proxies and web unblockers.\n",
      "Everything you need to scrape data at scale is under the hood,\n",
      "making your web scrapers unstoppable. Try Bright Data's Scraping Browser API for free right now with the link in the description.\n",
      "But the next big AI tool you need to know about is Stable Diffusion 3 Medium.\n",
      "The model weights were just released hours ago, and it's the most advanced open text to image model out there.\n",
      "Unfortunately, it's only available under a non-commercial license, but the level of quality is pretty amazing,\n",
      "and can now reliably generate text from your prompts. Now if you currently have an AI girlfriend,\n",
      "I would highly recommend upgrading her to this new model. It's the personality that matters,\n",
      "but she's going to be looking pretty mid compared to all your friends on ST3.\n",
      "Now another tool that's actually useful is this sound effect generator from Eleven Labs,\n",
      "the same company that engineered my voice. All you have to do is describe what you want to hear,\n",
      "and it will generate multiple sound effects. Here's an example of two different results.\n",
      "And example two. Now what I failed to tell you is that one example was real,\n",
      "and one was AI generated. I bet you can't smell the difference,\n",
      "but now we need to talk about code generation. I've been patiently waiting for AI to take my programming job,\n",
      "but so far I've been disappointed. However, there may still be hope.\n",
      "A few weeks ago, the French startup Mistral released a new model called CodeStroll.\n",
      "It's also an open model, but cannot be used for commercial purposes yet,\n",
      "and performs extremely well on coding benchmarks compared to other open models.\n",
      "I've been playing around with it on Ollama, and while it's extremely impressive,\n",
      "and also very fast, it still struggles with the age old problem of centering a div.\n",
      "Now when it comes to AI writing code, there are two types of people.\n",
      "There's people like Devin who are doing AI maxing, and trying to get AI to write nearly 100% of our code.\n",
      "These people are usually young and naive, according to the AI doomers on the other end of the spectrum.\n",
      "They're usually boomers who think that AI code is total slop,\n",
      "and has no place in the industry. But the optimal hot take is likely somewhere in between.\n",
      "Another free tool you can use right now is Cursor, which is a fork of VS Code,\n",
      "and one of the first truly AI focused code editors. Instead of memorizing syntax,\n",
      "give it the context of an existing code base or documentation,\n",
      "then hit Ctrl-K and write your code with natural language. And if you're worried that it's writing garbage code,\n",
      "you can enforce certain rules and even have it perform a code review.\n",
      "It's like GitHub Copilot on steroids. Generative AI still has a long way to go,\n",
      "but if I were Jada right now, I'd be very concerned about the amount of progress made by these data science nerds in just the last year.\n",
      "This has been the Code Report. Thanks for watching, and I will see you in the next one.\n",
      " > ===========================\n",
      "wən jɪɹ əˈgoʊ, ðɪs ˌənbəˈlivəbəɫ ˈvɪdioʊ əv wɪɫ smɪθ ˈitɪŋ spəˈgɛti tʊk ðə wəɹɫd baɪ stɔɹm.\n",
      " length:91\n",
      " length:91\n",
      "wi ˈjumənz dʒoʊkt əˈbaʊt ɪt. wi kʊd ˈizəli tɛɫ ðət ɪt wɑz feɪk,\n",
      " length:63\n",
      " length:63\n",
      "ənd æt ðət pɔɪnt, ˈnoʊˌbɑˌdi wɑz ˈɹɪli əˈfɹeɪd. bət fæst ˈfɔɹwəɹd wən jɪɹ ˈleɪtəɹ,\n",
      " length:82\n",
      " length:82\n",
      "ənd ˈdʒɛnəɹətɪv eɪaɪ tɛk həz ˈteɪkən əˈnəðəɹ judʒ lip ˈfɔɹwəɹd. wɪɫ smɪθ ˈitɪŋ spəˈgɛti ɪn tˈwɛnti tˈwɛntiˌfɔɹ ɪz ˈnəθɪŋ tɪ dʒoʊk əɹaʊnd əˈbaʊt.\n",
      " length:144\n",
      " length:144\n",
      "ɪf ɪt ˈdəzənt plæˈtoʊ sun, ɪt kʊd pʊt ɑɹ ˈhɑliˌwʊd ˈaɪdəɫz aʊt əv ˈbɪznɪs,\n",
      " length:74\n",
      " length:74\n",
      "ənd ðɛɹ wʊd bi noʊ wən lɛft tɪ ˈbɹeɪnˌwɑʃ ˈjuˈɛs. ɪn ˈtudeɪz ˈvɪdioʊ,\n",
      " length:69\n",
      " length:69\n",
      "wɪɫ dɪˈsɛnd ˈfəɹðəɹ ˈɪntu ənˈkæni ˈvæli, ənd lʊk æt faɪv nu ˈdʒɛnəɹətɪv eɪaɪ tuɫz ðət ju kən ˈæˌktʃuəli juz təˈdeɪ.\n",
      " length:115\n",
      " length:115\n",
      "baɪ ðə ɛnd əv ðɪs ˈvɪdioʊ, juɫ bi ˈeɪbəɫ tɪ faɪəɹ jʊɹ ˈjumən fəˈtɑgɹəfəɹ,\n",
      " length:73\n",
      " length:73\n",
      "vɪdiˈɔgɹəfəɹ, saʊnd ˈɛndʒəˈnɪɹ, ənd ˈpɹoʊˌgɹæməɹ. ɪt ɪz dʒun ˈsɛvənˈtinθ, tˈwɛnti tˈwɛntiˌfɔɹ, ənd jʊɹ ˈwɑtʃɪŋ ðə koʊd ɹɪˈpɔɹt.\n",
      " length:127\n",
      " length:127\n",
      "ə fju mənθs əˈgoʊ, ˈoʊpən eɪaɪ ˈpɹivˌjud soɹa*, ənd tizd ˈjuˈɛs wɪθ ə bəntʃ əv eɪaɪ ˈvɪdioʊz.\n",
      " length:93\n",
      " length:93\n",
      "ˈgugəɫ ˈleɪtəɹ ˈfɑloʊd ðət əp wɪθ veeo*, wɪtʃ wɑz ˈɔlsoʊ kwaɪt ˌɪmˈpɹɛsɪv,\n",
      " length:74\n",
      " length:74\n",
      "bət dʒɪst ðɪs wik, ðə tʃaɪˈniz dɹɑpt ə nu ˈmɑdəɫ kɔɫd klɪŋ.\n",
      " length:59\n",
      " length:59\n",
      "ðeɪ kən ˈdʒɛnəɹˌeɪt ˈvɪdioʊz tu ˈmɪnəts lɔŋ, əp tɪ ˈθəɹˌdi fps*.\n",
      " length:64\n",
      " length:64\n",
      "ɪts ˌɪnˈkɹɛdəbli ˌɪmˈpɹɛsɪv, ənd ˈɑɹgjuəbli ˈbɛtəɹ ðən soɹa*. bət ðɛɹz wən bɪg ˈpɹɑbləm wɪθ ɔɫ əv ðiz ˈmɑdəɫz.\n",
      " length:110\n",
      " length:110\n",
      "ðɛɹ nɑt əˈveɪləbəɫ tɪ ðə ˈpəblɪk. wɛɫ ˈləkəli, ə nu tuɫ dʒɪst dɹɑpt kɔɫd ðə dɹim məˈʃin fɹəm ˈlumə læbz,\n",
      " length:104\n",
      " length:104\n",
      "ənd ɪt əˈlaʊz ju tɪ kɹiˈeɪt ˈɹɛlətɪvli ˌɹiəˈlɪstɪk ˈvɪdioʊ klɪps. aɪ ˈpɹɑmptɪd ɪt fəɹ tu oʊɫd mɛn duɪŋ ˈjoʊgə,\n",
      " length:110\n",
      " length:110\n",
      "ənd ðə ɹɪˈzəɫt ɪz ˌɪndɪˈstɪŋgwɪʃəbəɫ fɹəm ɹiɫ laɪf, ənˈlɛs əv kɔɹs ju lʊk ˈvɛɹi ˈkloʊsli æt ðə ˈfɪŋgəɹz.\n",
      " length:104\n",
      " length:104\n",
      "ɪn əˈdɪʃən, ðɪs ɪz ðə tuɫ juzd tɪ ˈdʒɛnəɹˌeɪt ə ˌɹiəˈlɪstɪk wɪɫ smɪθ ˈitɪŋ spəˈgɛti,\n",
      " length:84\n",
      " length:84\n",
      "ənd waɪɫ ˌɪmˈpɹɛsɪv, ðɛɹz ˈɹɪli noʊ ˈpɹæktɪkəɫ əɹ kəˈməɹʃəɫ juz fəɹ ðɪs tuɫ jɛt.\n",
      " length:80\n",
      " length:80\n",
      "ɔɫ ɪts ˈɹɪli gʊd æt ɪz ˈsɪmjəˌleɪtɪŋ ˈnaɪtˌmɛɹz. ˌbiˈfɔɹ wi muv ɔn tɪ ðə nɛkst ˈtɑpɪk ðoʊ,\n",
      " length:90\n",
      " length:90\n",
      "lɛts tɔk əˈbaʊt ˈsəmθɪŋ jʊɹ ɫlms* ənd eɪaɪ ˈmɑdəɫz kænt lɪv wɪˈθaʊt,\n",
      " length:68\n",
      " length:68\n",
      "ˈdætə. ˈdætə kəˈlɛkʃən ɔn ðə wɛb juzd tɪ bi ə ˈnaɪtˌmɛɹ.\n",
      " length:56\n",
      " length:56\n",
      "jʊd hæv tɪ sɛt əp ˈpɹɑksi ˈnɛtˌwəɹks, dɪˈvɛləp wɛb unblockeɹs* tɪ ˈbaɪˌpæs captchas*,\n",
      " length:85\n",
      " length:82\n",
      "diɫ wɪθ ˈsəɹvəɹ ˈɛɹəɹz, ˈbɹaʊzəɹ ˈfɪŋgəɹˌpɹɪntɪŋ, ənd ɔɫ sɔɹts əv ˈəðəɹ ˈɪʃuz.\n",
      " length:78\n",
      " length:78\n",
      "bət wɪθ ˌɹɛzɪˈdɛnʃəɫ ˈpɹɑksiz ənd wɛb ɔtəˈmeɪʃən tuɫz laɪk səˈliniəm, pəpəˈtiɹ,\n",
      " length:79\n",
      " length:79\n",
      "ənd ˈpleɪˌɹaɪt, ju kən skɹeɪp ðə wɛb ɔn ə ˈmæsɪv skeɪɫ,\n",
      " length:55\n",
      " length:55\n",
      "wɪˈθaʊt bloʊɪŋ əp jʊɹ ˈbədʒɪt ɪn ðə ˈpɹɔˌsɛs. ðæts wɛɹ bɹaɪt ˈdætə kəmz ɪn,\n",
      " length:75\n",
      " length:75\n",
      "ðə ˈspɑnsəɹ əv ˈtudeɪz ˈvɪdioʊ. ðeɪ teɪk jʊɹ skɹeɪp ɑps tɪ ðə nɛkst ˈlɛvəɫ æt ə ˈfɹækʃən əv ðə kɔst.\n",
      " length:100\n",
      " length:100\n",
      "wɪθ ðɛɹ ˈskɹeɪpɪŋ ˈbɹaʊzəɹ api*, ju kən fəɹˈgɛt əˈbaʊt ˈpɹɑksiz ənd wɛb unblockeɹs*.\n",
      " length:84\n",
      " length:83\n",
      "ˈɛvɹiˌθɪŋ ju nid tɪ skɹeɪp ˈdætə æt skeɪɫ ɪz ˈəndəɹ ðə hʊd,\n",
      " length:59\n",
      " length:59\n",
      "ˈmeɪkɪŋ jʊɹ wɛb ˈskɹeɪpəɹz ənˈstɑpəbəɫ. tɹaɪ bɹaɪt ˈdætəz ˈskɹeɪpɪŋ ˈbɹaʊzəɹ api* fəɹ fɹi ɹaɪt naʊ wɪθ ðə lɪŋk ɪn ðə dɪˈskɹɪpʃən.\n",
      " length:129\n",
      " length:129\n",
      "bət ðə nɛkst bɪg eɪaɪ tuɫ ju nid tɪ noʊ əˈbaʊt ɪz ˈsteɪbəɫ dɪfˈjuʒən θɹi ˈmidiəm.\n",
      " length:81\n",
      " length:81\n",
      "ðə ˈmɑdəɫ weɪts wəɹ dʒɪst ɹiˈlist aʊəɹz əˈgoʊ, ənd ɪts ðə moʊst ədˈvænst ˈoʊpən tɛkst tɪ ˈɪmɪdʒ ˈmɑdəɫ aʊt ðɛɹ.\n",
      " length:111\n",
      " length:111\n",
      "ənˈfɔɹtʃənətli, ɪts ˈoʊnli əˈveɪləbəɫ ˈəndəɹ ə ˈnɑnkəˈməɹʃəɫ ˈlaɪsəns, bət ðə ˈlɛvəɫ əv kˈwɑləti ɪz ˈpɹɪti əˈmeɪzɪŋ,\n",
      " length:116\n",
      " length:116\n",
      "ənd kən naʊ ɹɪˈlaɪəbli ˈdʒɛnəɹˌeɪt tɛkst fɹəm jʊɹ pɹɑmpts. naʊ ɪf ju ˈkəɹəntli hæv ən eɪaɪ ˈgəɹlˌfɹɛnd,\n",
      " length:103\n",
      " length:103\n",
      "aɪ wʊd ˈhaɪli ˌɹɛkəˈmɛnd ˈəpˌgɹeɪdɪŋ həɹ tɪ ðɪs nu ˈmɑdəɫ. ɪts ðə ˌpəɹsəˈnælɪti ðət ˈmætəɹz,\n",
      " length:92\n",
      " length:92\n",
      "bət ʃiz goʊɪŋ tɪ bi ˈlʊkɪŋ ˈpɹɪti mɪd kəmˈpɛɹd tɪ ɔɫ jʊɹ fɹɛndz ɔn stthɹee*.\n",
      " length:76\n",
      " length:76\n",
      "naʊ əˈnəðəɹ tuɫ ðæts ˈæˌktʃuəli ˈjusfəɫ ɪz ðɪs saʊnd ˈifɛkt ˈdʒɛnəɹˌeɪtəɹ fɹəm ˈilɛvən læbz,\n",
      " length:92\n",
      " length:92\n",
      "ðə seɪm ˈkəmpəˌni ðət ˌɛndʒəˈniɹd maɪ vɔɪs. ɔɫ ju hæv tɪ du ɪz dɪˈskɹaɪb wət ju wɔnt tɪ hiɹ,\n",
      " length:92\n",
      " length:92\n",
      "ənd ɪt wɪɫ ˈdʒɛnəɹˌeɪt ˈməltəpəɫ saʊnd ˈifɛkts. hɪɹz ən ɪgˈzæmpəɫ əv tu ˈdɪfəɹənt ɹɪˈzəɫts.\n",
      " length:91\n",
      " length:91\n",
      "ənd ɪgˈzæmpəɫ tu. naʊ wət aɪ feɪɫd tɪ tɛɫ ju ɪz ðət wən ɪgˈzæmpəɫ wɑz ɹiɫ,\n",
      " length:74\n",
      " length:74\n",
      "ənd wən wɑz eɪaɪ ˈdʒɛnəɹˌeɪtɪd. aɪ bɛt ju kænt smɛɫ ðə ˈdɪfəɹəns,\n",
      " length:65\n",
      " length:65\n",
      "bət naʊ wi nid tɪ tɔk əˈbaʊt koʊd ˌdʒɛnəɹˈeɪʃən. aɪv bɪn ˈpeɪʃəntli ˈweɪtɪŋ fəɹ eɪaɪ tɪ teɪk maɪ ˈpɹoʊˌgɹæmɪŋ dʒɑb,\n",
      " length:115\n",
      " length:115\n",
      "bət soʊ fɑɹ aɪv bɪn ˌdɪsəˈpɔɪnɪd. ˌhaʊˈɛvəɹ, ðɛɹ meɪ stɪɫ bi hoʊp.\n",
      " length:66\n",
      " length:66\n",
      "ə fju wiks əˈgoʊ, ðə fɹɛntʃ ˈstɑɹˌtəp ˈmɪstɹəɫ ɹiˈlist ə nu ˈmɑdəɫ kɔɫd koʊd stɹoʊɫ.\n",
      " length:84\n",
      " length:84\n",
      "ɪts ˈɔlsoʊ ən ˈoʊpən ˈmɑdəɫ, bət ˈkænɑt bi juzd fəɹ kəˈməɹʃəɫ ˈpəɹpəsɪz jɛt,\n",
      " length:76\n",
      " length:76\n",
      "ənd pəɹˈfɔɹmz ɪkˈstɹimli wɛɫ ɔn ˈkoʊdɪŋ ˈbɛntʃˌmɑɹks kəmˈpɛɹd tɪ ˈəðəɹ ˈoʊpən ˈmɑdəɫz.\n",
      " length:86\n",
      " length:86\n",
      "aɪv bɪn pleɪɪŋ əɹaʊnd wɪθ ɪt ɔn ollama*, ənd waɪɫ ɪts ɪkˈstɹimli ˌɪmˈpɹɛsɪv,\n",
      " length:76\n",
      " length:76\n",
      "ənd ˈɔlsoʊ ˈvɛɹi fæst, ɪt stɪɫ ˈstɹəgəɫz wɪθ ðə eɪdʒ oʊɫd ˈpɹɑbləm əv ˈsɛntəɹɪŋ ə div*.\n",
      " length:87\n",
      " length:87\n",
      "naʊ wɪn ɪt kəmz tɪ eɪaɪ ˈɹaɪtɪŋ koʊd, ðɛɹ əɹ tu taɪps əv ˈpipəɫ.\n",
      " length:64\n",
      " length:64\n",
      "ðɛɹz ˈpipəɫ laɪk ˈdɛvɪn hu əɹ duɪŋ eɪaɪ maxing*, ənd tɹaɪɪŋ tɪ gɪt eɪaɪ tɪ ɹaɪt ˈnɪɹli wən ˈhənəɹd% əv ɑɹ koʊd.\n",
      " length:111\n",
      " length:110\n",
      "ðiz ˈpipəɫ əɹ ˈjuʒəwəli jəŋ ənd naɪiv, əˈkɔɹdɪŋ tɪ ðə eɪaɪ doomeɹs* ɔn ðə ˈəðəɹ ɛnd əv ðə ˈspɛktɹəm.\n",
      " length:100\n",
      " length:100\n",
      "ðɛɹ ˈjuʒəwəli ˈbuməɹz hu θɪŋk ðət eɪaɪ koʊd ɪz ˈtoʊtəɫ slɑp,\n",
      " length:60\n",
      " length:60\n",
      "ənd həz noʊ pleɪs ɪn ðə ˈɪndəstɹi. bət ðə ˈɑptɪməɫ hɑt teɪk ɪz ˈlaɪkli ˈsəmˌwɛɹ ɪn bɪtˈwin.\n",
      " length:91\n",
      " length:91\n",
      "əˈnəðəɹ fɹi tuɫ ju kən juz ɹaɪt naʊ ɪz ˈkəɹsəɹ, wɪtʃ ɪz ə fɔɹk əv ˈvəɹsəz koʊd,\n",
      " length:79\n",
      " length:79\n",
      "ənd wən əv ðə fəɹst ˈtɹuli eɪaɪ ˈfoʊkɪst koʊd ˈɛdɪtəɹz. ˌɪnˈstɛd əv ˈmɛməɹˌaɪzɪŋ ˈsɪnˌtæks,\n",
      " length:91\n",
      " length:91\n",
      "gɪv ɪt ðə ˈkɑntɛkst əv ən ɪgˈzɪstɪŋ koʊd beɪs əɹ ˌdɑkjəmɛnˈteɪʃən,\n",
      " length:66\n",
      " length:66\n",
      "ðɛn hɪt ctɹɫ-k* ənd ɹaɪt jʊɹ koʊd wɪθ ˈnætʃəɹəɫ ˈlæŋgwɪdʒ. ənd ɪf jʊɹ ˈwəɹid ðət ɪts ˈɹaɪtɪŋ ˈgɑɹbɪdʒ koʊd,\n",
      " length:107\n",
      " length:106\n",
      "ju kən ɛnˈfɔɹs ˈsəɹtən ɹuɫz ənd ˈivɪn hæv ɪt pəɹˈfɔɹm ə koʊd ˌɹivˈju.\n",
      " length:69\n",
      " length:69\n",
      "ɪts laɪk git* həb ˈkoʊpaɪlət ɔn ˈstɛɹɔɪdz. ˈdʒɛnəɹətɪv eɪaɪ stɪɫ həz ə lɔŋ weɪ tɪ goʊ,\n",
      " length:86\n",
      " length:86\n",
      "bət ɪf aɪ wəɹ jada* ɹaɪt naʊ, aɪd bi ˈvɛɹi kənˈsəɹnd əˈbaʊt ðə əˈmaʊnt əv ˈpɹɑˌgɹɛs meɪd baɪ ðiz ˈdætə saɪəns nəɹdz ɪn dʒɪst ðə læst jɪɹ.\n",
      " length:137\n",
      " length:137\n",
      "ðɪs həz bɪn ðə koʊd ɹɪˈpɔɹt. θæŋks fəɹ ˈwɑtʃɪŋ, ənd aɪ wɪɫ si ju ɪn ðə nɛkst wən.\n",
      " length:81\n",
      " length:81\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 850.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 10.43 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Run the tone color converter\u001b[39;00m\n\u001b[0;32m     20\u001b[0m encode_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m@MyShell\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtone_color_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_src_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc_se\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_se\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_se\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_se\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Dev\\ariolas-tech\\rnd\\OpenVoice\\openvoice\\api.py:154\u001b[0m, in \u001b[0;36mToneColorConverter.convert\u001b[1;34m(self, audio_src_path, src_se, tgt_se, output_path, tau, message)\u001b[0m\n\u001b[0;32m    150\u001b[0m spec \u001b[38;5;241m=\u001b[39m spectrogram_torch(y, hps\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfilter_length,\n\u001b[0;32m    151\u001b[0m                         hps\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msampling_rate, hps\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mhop_length, hps\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mwin_length,\n\u001b[0;32m    152\u001b[0m                         center\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    153\u001b[0m spec_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([spec\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 154\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid_src\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_se\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid_tgt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_se\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\n\u001b[0;32m    155\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    156\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_watermark(audio, message)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Dev\\ariolas-tech\\rnd\\OpenVoice\\openvoice\\models.py:498\u001b[0m, in \u001b[0;36mSynthesizerTrn.voice_conversion\u001b[1;34m(self, y, y_lengths, sid_src, sid_tgt, tau)\u001b[0m\n\u001b[0;32m    496\u001b[0m z_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow(z, y_mask, g\u001b[38;5;241m=\u001b[39mg_src)\n\u001b[0;32m    497\u001b[0m z_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflow(z_p, y_mask, g\u001b[38;5;241m=\u001b[39mg_tgt, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 498\u001b[0m o_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_hat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg_tgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_g\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg_tgt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m o_hat, y_mask, (z, z_p, z_hat)\n",
      "File \u001b[1;32mc:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Dev\\ariolas-tech\\rnd\\OpenVoice\\openvoice\\models.py:285\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x, g)\u001b[0m\n\u001b[0;32m    283\u001b[0m             xs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresblocks[i \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kernels \u001b[38;5;241m+\u001b[39m j](x)\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 285\u001b[0m             xs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_kernels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m     x \u001b[38;5;241m=\u001b[39m xs \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_kernels\n\u001b[0;32m    287\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n",
      "File \u001b[1;32mc:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Dev\\ariolas-tech\\rnd\\OpenVoice\\openvoice\\modules.py:298\u001b[0m, in \u001b[0;36mResBlock1.forward\u001b[1;34m(self, x, x_mask)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m c1, c2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs2):\n\u001b[1;32m--> 298\u001b[0m         xt \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLRELU_SLOPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    300\u001b[0m             xt \u001b[38;5;241m=\u001b[39m xt \u001b[38;5;241m*\u001b[39m x_mask\n",
      "File \u001b[1;32mc:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\functional.py:1677\u001b[0m, in \u001b[0;36mleaky_relu\u001b[1;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[0;32m   1675\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu_(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1677\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleaky_relu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegative_slope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 850.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 10.43 GiB is allocated by PyTorch, and 1.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "save_path = f\"{output_dir}/output_en_default.wav\"\n",
    "\n",
    "# Run the base speaker tts\n",
    "# text = \"This audio is generated by OpenVoice.\"\n",
    "text_src = \"resources/texts/en.txt\"\n",
    "\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "text = read_text_from_file(text_src)\n",
    "print(\"Text resource: \", text)\n",
    "\n",
    "src_path = f\"{output_dir}/tmp.wav\"\n",
    "base_speaker_tts.tts(text, src_path, speaker=\"default\", language=\"English\", speed=1.0)\n",
    "\n",
    "# Run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path,\n",
    "    src_se=source_se,\n",
    "    tgt_se=target_se,\n",
    "    output_path=save_path,\n",
    "    message=encode_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ea28a",
   "metadata": {},
   "source": [
    "**Try with different styles and speed.** The style can be controlled by the `speaker` parameter in the `base_speaker_tts.tts` method. Available choices: friendly, cheerful, excited, sad, angry, terrified, shouting, whispering. Note that the tone color embedding need to be updated. The speed can be controlled by the `speed` parameter. Let's try whispering with speed 0.9.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd022d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taniyow\\AppData\\Local\\Temp\\ipykernel_19032\\3098055831.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  source_se = torch.load(f\"{ckpt_base}/en_style_se.pth\").to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text resource:  ### English Translation:\n",
      "One year ago, this unbelievable video of Will Smith eating spaghetti took the world by storm. We humans joked about it. We could easily tell that it was fake, and at that point, nobody was really afraid. But fast forward one year later, and generative AI tech has taken another huge leap forward. Will Smith eating spaghetti in 2024 is nothing to joke around about. If it doesn't plateau soon, it could put our Hollywood idols out of business, and there would be no one left to brainwash us. In today's video, we'll descend further into Uncanny Valley, and look at five new generative AI tools that you can actually use today. By the end of this video, you'll be able to fire your human photographer, videographer, sound engineer, and programmer. It is June 17th, 2024, and you're watching The Code Report.\n",
      "\n",
      "A few months ago, OpenAI previewed Sora, and teased us with a bunch of AI videos. Google later followed that up with Veeo, which was also quite impressive, but just this week, the Chinese dropped a new model called Kling. They can generate videos two minutes long, up to 30 FPS. It's incredibly impressive, and arguably better than Sora. But there's one big problem with all of these models. They're not available to the public. Well luckily, a new tool just dropped called the Dream Machine from LumaLabs, and it allows you to create relatively realistic video clips. I prompted it for two old men doing yoga, and the result is indistinguishable from real life, unless of course you look very closely at the fingers. \n",
      "\n",
      "In addition, this is the tool used to generate a realistic Will Smith eating spaghetti, and while impressive, there's really no practical or commercial use for this tool yet. All it's really good at is simulating nightmares. Before we move on to the next topic though, let's talk about something your LLMs and AI models can't live without, data. Data collection on the web used to be a nightmare. You'd have to set up proxy networks, develop web unblockers to bypass CAPTCHAs, deal with server errors, browser fingerprinting, and all sorts of other issues. But with residential proxies and web automation tools like Selenium, Puppeteer, and Playwright, you can scrape the web on a massive scale, without blowing up your budget in the process. That's where Bright Data comes in, the sponsor of today's video. They take your scrape ops to the next level at a fraction of the cost. With their Scraping Browser API, you can forget about proxies and web unblockers. Everything you need to scrape data at scale is under the hood, making your web scrapers unstoppable. Try Bright Data's Scraping Browser API for free right now with the link in the description. \n",
      "\n",
      "But the next big AI tool you need to know about is Stable Diffusion 3 Medium. The model weights were just released hours ago, and it's the most advanced open text to image model out there. Unfortunately, it's only available under a non-commercial license, but the level of quality is pretty amazing, and can now reliably generate text from your prompts. Now if you currently have an AI girlfriend, I would highly recommend upgrading her to this new model. It's the personality that matters, but she's going to be looking pretty mid compared to all your friends on ST3. Now another tool that's actually useful is this sound effect generator from Eleven Labs, the same company that engineered my voice. All you have to do is describe what you want to hear, and it will generate multiple sound effects. Here's an example of two different results. And example two. \n",
      "\n",
      "Now what I failed to tell you is that one example was real, and one was AI generated. I bet you can't smell the difference, but now we need to talk about code generation. I've been patiently waiting for AI to take my programming job, but so far I've been disappointed. However, there may still be hope. A few weeks ago, the French startup Mistral released a new model called CodeStroll. It's also an open model, but cannot be used for commercial purposes yet, and performs extremely well on coding benchmarks compared to other open models. I've been playing around with it on Ollama, and while it's extremely impressive, and also very fast, it still struggles with the age old problem of centering a div. Now when it comes to AI writing code, there are two types of people. There's people like Devin who are doing AI maxing, and trying to get AI to write nearly 100% of our code. These people are usually young and naive, according to the AI doomers on the other end of the spectrum. They're usually boomers who think that AI code is total slop, and has no place in the industry. But the optimal hot take is likely somewhere in between. Another free tool you can use right now is Cursor, which is a fork of VS Code, and one of the first truly AI focused code editors. Instead of memorizing syntax, give it the context of an existing code base or documentation, then hit Ctrl-K and write your code with natural language. And if you're worried that it's writing garbage code, you can enforce certain rules and even have it perform a code review.\n",
      "\n",
      "It's like GitHub Copilot on steroids. Generative AI still has a long way to go, but if I were Jada right now, I'd be very concerned about the amount of progress made by these data science nerds in just the last year. This has been the Code Report. Thanks for watching, and I will see you in the next one.\n",
      "\n",
      "### Spanish Translation:\n",
      "Hace un año, este increíble video de Will Smith comiendo espaguetis sorprendió al mundo. Los humanos bromeábamos al respecto. Podíamos decir fácilmente que era falso, y en ese momento, nadie estaba realmente asustado. Pero avanzando un año, la tecnología de IA generativa ha dado otro gran salto adelante. Will Smith comiendo espaguetis en 2024 no es motivo de broma. Si no se estabiliza pronto, podría poner a nuestros ídolos de Hollywood fuera del negocio, y no quedaría nadie para lavarnos el cerebro. En el video de hoy, descenderemos más en el Valle Inquietante, y veremos cinco nuevas herramientas de IA generativa que puedes usar hoy en día. Al final de este video, podrás despedir a tu fotógrafo humano, videógrafo, ingeniero de sonido y programador. Es el 17 de junio de 2024, y estás viendo The Code Report.\n",
      "\n",
      "Hace unos meses, OpenAI presentó Sora y nos intrigó con una serie de videos de IA. Google después siguió con Veeo, que también fue bastante impresionante, pero justo esta semana, los chinos lanzaron un nuevo modelo llamado Kling. Pueden generar videos de dos minutos de duración, hasta 30 FPS. Es increíblemente impresionante y, posiblemente, mejor que Sora. Pero hay un gran problema con todos estos modelos. No están disponibles para el público. Pues afortunadamente, acaba de salir una nueva herramienta llamada Dream Machine de LumaLabs, que te permite crear clips de video relativamente realistas. Le pedí que generara dos hombres mayores haciendo yoga, y el resultado es indistinguible de la vida real, a menos que, por supuesto, mires muy de cerca los dedos.\n",
      "\n",
      "Además, esta es la herramienta utilizada para generar un Will Smith comiendo espaguetis realista, y aunque es impresionante, realmente no tiene un uso práctico o comercial todavía. Todo lo que realmente sabe hacer es simular pesadillas. Antes de pasar al siguiente tema, hablemos de algo de lo que tus LLMs y modelos de IA no pueden prescindir, datos. La recopilación de datos en la web solía ser una pesadilla. Tenías que configurar redes de proxy, desarrollar desbloqueadores web para sortear CAPTCHAs, lidiar con errores de servidor, huellas digitales de navegador y todo tipo de otros problemas. Pero con proxies residenciales y herramientas de automatización web como Selenium, Puppeteer y Playwright, puedes rastrear la web a una escala masiva, sin consumir tu presupuesto en el proceso. Ahí es donde entra Bright Data, el patrocinador del video de hoy. Llevan tus operaciones de rastreo al siguiente nivel a una fracción del costo. Con su API Scraping Browser, puedes olvidarte de los proxies y desbloqueadores web. Todo lo que necesitas para raspar datos a escala está bajo el capó, haciendo que tus raspadores web sean imparables. Prueba la API Scraping Browser de Bright Data gratis ahora con el enlace en la descripción.\n",
      "\n",
      "Pero la siguiente gran herramienta de IA que necesitas conocer es Stable Diffusion 3 Medium. Los pesos del modelo se acaban de liberar hace unas horas, y es el modelo de texto a imagen abierto más avanzado que existe. Lamentablemente, solo está disponible bajo una licencia no comercial, pero el nivel de calidad es bastante increíble y ahora puede generar texto de manera confiable a partir de tus indicaciones. Ahora, si actualmente tienes una novia de IA, te recomendaría encarecidamente actualizarla a este nuevo modelo. La personalidad es lo que importa, pero se verá bastante mediocre en comparación con todas tus amigas en ST3. Ahora, otra herramienta que realmente es útil es este generador de efectos de sonido de Eleven Labs, la misma compañía que diseñó mi voz. Todo lo que tienes que hacer es describir lo que quieres oír, y generará múltiples efectos de sonido. Aquí hay un ejemplo de dos resultados diferentes. Y el ejemplo dos.\n",
      "\n",
      "Ahora, lo que no te dije es que un ejemplo era real y otro fue generado por IA. Apuesto a que no puedes notar la diferencia, pero ahora necesitamos hablar sobre la generación de código. He estado esperando pacientemente para que la IA tome mi trabajo de programación, pero hasta ahora he estado decepcionado. Sin embargo, todavía puede haber esperanza. Hace unas semanas, la startup francesa Mistral lanzó un nuevo modelo llamado CodeStroll. Es también un modelo abierto, pero aún no se puede usar con fines comerciales y rinde extremadamente bien en puntos de referencia de codificación en comparación con otros modelos abiertos. He estado jugando con él en Ollama, y aunque es extremadamente impresionante y también muy rápido, todavía lucha con el problema antiguo de centrar un div. Ahora, cuando se trata de AI escribiendo código, hay dos tipos de personas. Están las personas como Devin que están maximizando la AI y tratando de hacer que la AI escriba casi el 100% de nuestro código. Estas personas suelen ser jóvenes e ingenuas, según los pesimistas de la IA en el otro extremo del espectro. Estos suelen ser boomers que piensan que el código de AI es totalmente basura y no tiene lugar en la industria. Pero la opinión óptima probablemente esté en algún punto intermedio. Otra herramienta gratuita que puedes usar ahora mismo es Cursor, que es una bifurcación de VS Code, y uno de los primeros editores de código verdaderamente enfocados en IA. En lugar de memorizar la sintaxis, dale el contexto de una base de código existente o documentación, luego presiona Ctrl-K y escribe tu código en lenguaje natural. Y si te preocupa que esté escribiendo código basura, puedes imponer ciertas reglas e incluso hacer que realice una revisión de código.\n",
      "\n",
      "Es como GitHub Copilot a lo grande. La IA generativa todavía tiene un largo camino por recorrer, pero si yo fuera Jada ahora mismo, estaría muy preocupado por la cantidad de progreso que han logrado estos nerds de la ciencia de datos en solo el último año. Esto ha sido The Code Report. Gracias por ver, y nos vemos en el próximo.\n",
      "\n",
      "### French Translation:\n",
      "Il y a un an, cette vidéo incroyable de Will Smith mangeant des spaghettis a fait sensation dans le monde entier. Les humains en plaisantions. Nous pouvions facilement dire que c'était faux, et à ce moment-là, personne n'avait vraiment peur. Mais avancez d'un an, et la technologie de l'IA générative a fait un saut énorme. Will Smith mangeant des spaghettis en 2024 n'est pas à prendre à la légère. Si cela ne se stabilise pas bientôt, cela pourrait mettre nos idoles d'Hollywood hors affaires, et il n'y aurait plus personne pour nous laver le cerveau. Dans la vidéo d'aujourd'hui, nous plongerons plus loin dans la vallée de l'étrange et examinerons cinq nouveaux outils d'IA générative que vous pouvez vraiment utiliser aujourd'hui. À la fin de cette vidéo, vous serez en mesure de licencier votre photographe humain, vidéaste, ingénieur du son et programmeur. Nous sommes le 17 juin 2024, et vous regardez The Code Report.\n",
      "\n",
      "Il y a quelques mois, OpenAI a présenté Sora et nous a alléchés avec une série de vidéos d'IA. Google a suivi avec Veeo, qui était également assez impressionnant, mais juste cette semaine, les Chinois ont lancé un nouveau modèle appelé Kling. Ils peuvent générer des vidéos de deux minutes de long, jusqu'à 30 FPS. C'est incroyablement impressionnant et probablement meilleur que Sora. Mais il y a un gros problème avec tous ces modèles. Ils ne sont pas disponibles pour le public. Heureusement, un nouvel outil vient de sortir appelé Dream Machine de LumaLabs, et il vous permet de créer des clips vidéo relativement réalistes. Je lui ai demandé de générer deux vieux hommes faisant du yoga, et le résultat est indiscernable de la vie réelle, à moins bien sûr de regarder de très près les doigts.\n",
      "\n",
      "De plus, c'est l'outil qui a été utilisé pour générer un Will Smith mangeant des spaghettis réaliste, et bien que ce soit impressionnant, il n'y a vraiment pas encore d'utilisation pratique ou commerciale pour cet outil. Tout ce qu'il fait vraiment bien est de simuler des cauchemars. Avant de passer au sujet suivant, parlons d'une chose dont vos LLM et modèles d'IA ne peuvent se passer, les données. La collecte de données sur le web était autrefois un cauchemar. Vous deviez configurer des réseaux proxy, développer des débloqueurs web pour contourner les CAPTCHA, gérer des erreurs de serveur, des empreintes digitales des navigateurs et toutes sortes d'autres problèmes. Mais avec des proxies résidentiels et des outils d'automatisation web comme Selenium, Puppeteer et Playwright, vous pouvez scraper le web à une échelle massive, sans exploser votre budget dans le processus. C'est là que Bright Data entre en jeu, le sponsor de la vidéo d'aujourd'hui. Ils portent vos opérations de scrape au niveau supérieur à une fraction du coût. Avec leur API Scraping Browser, vous pouvez oublier les proxies et les débloqueurs web. Tout ce dont vous avez besoin pour scraper des données à grande échelle est sous le capot, rendant vos scrapers web imparables. Essayez l'API Scraping Browser de Bright Data gratuitement maintenant avec le lien dans la description.\n",
      "\n",
      "Mais le prochain grand outil d'IA que vous devez connaître est Stable Diffusion 3 Medium. Les poids du modèle viennent d'être publiés il y a quelques heures, et c'est le modèle de texte à image le plus avancé disponible. Malheureusement, il n'est disponible que sous une licence non commerciale, mais le niveau de qualité est assez incroyable et peut désormais générer de manière fiable du texte à partir de vos invites. Maintenant, si vous avez actuellement une petite amie IA, je vous recommande vivement de la mettre à niveau vers ce nouveau modèle. C'est la personnalité qui compte, mais elle va paraître assez moyenne par rapport à toutes vos amies sur ST3. Maintenant, un autre outil qui est vraiment utile est ce générateur d'effets sonores de Eleven Labs, la même entreprise qui a conçu ma voix. Tout ce que vous avez à faire est de décrire ce que vous voulez entendre et il générera plusieurs effets sonores. Voici un exemple de deux résultats différents. Et exemple deux.\n",
      "\n",
      "Maintenant ce que je ne vous ai pas dit, c'est qu'un exemple était réel et l'autre était généré par l'IA. Je parie que vous ne pouvez pas sentir la différence, mais maintenant, nous devons parler de la génération de code. J'ai attendu patiemment que l'IA prenne mon travail de programmation, mais jusqu'à présent, j'ai été déçu. Cependant, il peut encore y avoir de l'espoir. Il y a quelques semaines, la startup française Mistral a lancé un nouveau modèle appelé CodeStroll. C'est aussi un modèle open source, mais il ne peut pas encore être utilisé à des fins commerciales, et il performe extrêmement bien sur les benchmarks de codage par rapport à d'autres modèles open source. J'ai joué avec sur Ollama, et bien qu'il soit extrêmement impressionnant et également très rapide, il a encore du mal avec le vieux problème de centrer un div. Maintenant, quand il s'agit de l'IA écrivant du code, il y a deux types de personnes. Il y a les gens comme Devin qui maximisent l'IA, essayant de faire écrire près de 100% de notre code par l'IA. Ces personnes sont généralement jeunes et naïves selon les pessimistes de l'IA à l'autre bout du spectre. Ce sont généralement des boomers qui pensent que le code IA est total slop et n'a pas sa place dans l'industrie. Mais l'avis optimal se situe probablement quelque part entre les deux. Un autre outil gratuit que vous pouvez utiliser dès maintenant est Cursor, qui est un fork de VS Code, et l'un des premiers éditeurs de code réellement axés sur l'IA. Au lieu de mémoriser la syntaxe, donnez-lui le contexte d'une base de code existante ou de la documentation, puis appuyez sur Ctrl-K et écrivez votre code avec le langage naturel. Et si vous craignez qu'il écrive du code pourri, vous pouvez imposer certaines règles et même lui faire effectuer une revue de code.\n",
      "\n",
      "C'est comme GitHub Copilot sous stéroïdes. L'IA générative a encore un long chemin à parcourir, mais si j'étais Jada en ce moment, je serais très préoccupé par la quantité de progrès réalisés par ces nerds de la science des données en seulement la dernière année. C'était The Code Report. Merci de regarder, et je vous verrai dans le prochain.\n",
      " > Text splitted to sentences.\n",
      "### English Translation: One year ago, this unbelievable video of Will Smith eating spaghetti took the world by storm.\n",
      "We humans joked about it. We could easily tell that it was fake,\n",
      "and at that point, nobody was really afraid. But fast forward one year later,\n",
      "and generative AI tech has taken another huge leap forward. Will Smith eating spaghetti in 2024 is nothing to joke around about.\n",
      "If it doesn't plateau soon, it could put our Hollywood idols out of business,\n",
      "and there would be no one left to brainwash us. In today's video,\n",
      "we'll descend further into Uncanny Valley, and look at five new generative AI tools that you can actually use today.\n",
      "By the end of this video, you'll be able to fire your human photographer,\n",
      "videographer, sound engineer, and programmer. It is June 17th, 2024, and you're watching The Code Report.\n",
      "A few months ago, OpenAI previewed Sora, and teased us with a bunch of AI videos.\n",
      "Google later followed that up with Veeo, which was also quite impressive,\n",
      "but just this week, the Chinese dropped a new model called Kling.\n",
      "They can generate videos two minutes long, up to 30 FPS.\n",
      "It's incredibly impressive, and arguably better than Sora. But there's one big problem with all of these models.\n",
      "They're not available to the public. Well luckily, a new tool just dropped called the Dream Machine from LumaLabs,\n",
      "and it allows you to create relatively realistic video clips. I prompted it for two old men doing yoga,\n",
      "and the result is indistinguishable from real life, unless of course you look very closely at the fingers.\n",
      "In addition, this is the tool used to generate a realistic Will Smith eating spaghetti,\n",
      "and while impressive, there's really no practical or commercial use for this tool yet.\n",
      "All it's really good at is simulating nightmares. Before we move on to the next topic though,\n",
      "let's talk about something your LLMs and AI models can't live without,\n",
      "data. Data collection on the web used to be a nightmare.\n",
      "You'd have to set up proxy networks, develop web unblockers to bypass CAPTCHAs,\n",
      "deal with server errors, browser fingerprinting, and all sorts of other issues.\n",
      "But with residential proxies and web automation tools like Selenium, Puppeteer,\n",
      "and Playwright, you can scrape the web on a massive scale,\n",
      "without blowing up your budget in the process. That's where Bright Data comes in,\n",
      "the sponsor of today's video. They take your scrape ops to the next level at a fraction of the cost.\n",
      "With their Scraping Browser API, you can forget about proxies and web unblockers.\n",
      "Everything you need to scrape data at scale is under the hood,\n",
      "making your web scrapers unstoppable. Try Bright Data's Scraping Browser API for free right now with the link in the description.\n",
      "But the next big AI tool you need to know about is Stable Diffusion 3 Medium.\n",
      "The model weights were just released hours ago, and it's the most advanced open text to image model out there.\n",
      "Unfortunately, it's only available under a non-commercial license, but the level of quality is pretty amazing,\n",
      "and can now reliably generate text from your prompts. Now if you currently have an AI girlfriend,\n",
      "I would highly recommend upgrading her to this new model. It's the personality that matters,\n",
      "but she's going to be looking pretty mid compared to all your friends on ST3.\n",
      "Now another tool that's actually useful is this sound effect generator from Eleven Labs,\n",
      "the same company that engineered my voice. All you have to do is describe what you want to hear,\n",
      "and it will generate multiple sound effects. Here's an example of two different results.\n",
      "And example two. Now what I failed to tell you is that one example was real,\n",
      "and one was AI generated. I bet you can't smell the difference,\n",
      "but now we need to talk about code generation. I've been patiently waiting for AI to take my programming job,\n",
      "but so far I've been disappointed. However, there may still be hope.\n",
      "A few weeks ago, the French startup Mistral released a new model called CodeStroll.\n",
      "It's also an open model, but cannot be used for commercial purposes yet,\n",
      "and performs extremely well on coding benchmarks compared to other open models.\n",
      "I've been playing around with it on Ollama, and while it's extremely impressive,\n",
      "and also very fast, it still struggles with the age old problem of centering a div.\n",
      "Now when it comes to AI writing code, there are two types of people.\n",
      "There's people like Devin who are doing AI maxing, and trying to get AI to write nearly 100% of our code.\n",
      "These people are usually young and naive, according to the AI doomers on the other end of the spectrum.\n",
      "They're usually boomers who think that AI code is total slop,\n",
      "and has no place in the industry. But the optimal hot take is likely somewhere in between.\n",
      "Another free tool you can use right now is Cursor, which is a fork of VS Code,\n",
      "and one of the first truly AI focused code editors. Instead of memorizing syntax,\n",
      "give it the context of an existing code base or documentation,\n",
      "then hit Ctrl-K and write your code with natural language. And if you're worried that it's writing garbage code,\n",
      "you can enforce certain rules and even have it perform a code review.\n",
      "It's like GitHub Copilot on steroids. Generative AI still has a long way to go,\n",
      "but if I were Jada right now, I'd be very concerned about the amount of progress made by these data science nerds in just the last year.\n",
      "This has been the Code Report. Thanks for watching, and I will see you in the next one.\n",
      "### Spanish Translation: Hace un año, este increíble video de Will Smith comiendo espaguetis sorprendió al mundo.\n",
      "Los humanos bromeábamos al respecto. Podíamos decir fácilmente que era falso,\n",
      "y en ese momento, nadie estaba realmente asustado. Pero avanzando un año,\n",
      "la tecnología de IA generativa ha dado otro gran salto adelante.\n",
      "Will Smith comiendo espaguetis en 2024 no es motivo de broma.\n",
      "Si no se estabiliza pronto, podría poner a nuestros ídolos de Hollywood fuera del negocio,\n",
      "y no quedaría nadie para lavarnos el cerebro. En el video de hoy,\n",
      "descenderemos más en el Valle Inquietante, y veremos cinco nuevas herramientas de IA generativa que puedes usar hoy en día.\n",
      "Al final de este video, podrás despedir a tu fotógrafo humano,\n",
      "videógrafo, ingeniero de sonido y programador. Es el 17 de junio de 2024,\n",
      "y estás viendo The Code Report. Hace unos meses, OpenAI presentó Sora y nos intrigó con una serie de videos de IA.\n",
      "Google después siguió con Veeo, que también fue bastante impresionante, pero justo esta semana,\n",
      "los chinos lanzaron un nuevo modelo llamado Kling. Pueden generar videos de dos minutos de duración,\n",
      "hasta 30 FPS. Es increíblemente impresionante y, posiblemente, mejor que Sora.\n",
      "Pero hay un gran problema con todos estos modelos. No están disponibles para el público.\n",
      "Pues afortunadamente, acaba de salir una nueva herramienta llamada Dream Machine de LumaLabs,\n",
      "que te permite crear clips de video relativamente realistas. Le pedí que generara dos hombres mayores haciendo yoga,\n",
      "y el resultado es indistinguible de la vida real, a menos que,\n",
      "por supuesto, mires muy de cerca los dedos. Además, esta es la herramienta utilizada para generar un Will Smith comiendo espaguetis realista,\n",
      "y aunque es impresionante, realmente no tiene un uso práctico o comercial todavía.\n",
      "Todo lo que realmente sabe hacer es simular pesadillas. Antes de pasar al siguiente tema,\n",
      "hablemos de algo de lo que tus LLMs y modelos de IA no pueden prescindir,\n",
      "datos. La recopilación de datos en la web solía ser una pesadilla.\n",
      "Tenías que configurar redes de proxy, desarrollar desbloqueadores web para sortear CAPTCHAs,\n",
      "lidiar con errores de servidor, huellas digitales de navegador y todo tipo de otros problemas.\n",
      "Pero con proxies residenciales y herramientas de automatización web como Selenium,\n",
      "Puppeteer y Playwright, puedes rastrear la web a una escala masiva,\n",
      "sin consumir tu presupuesto en el proceso. Ahí es donde entra Bright Data,\n",
      "el patrocinador del video de hoy. Llevan tus operaciones de rastreo al siguiente nivel a una fracción del costo.\n",
      "Con su API Scraping Browser, puedes olvidarte de los proxies y desbloqueadores web.\n",
      "Todo lo que necesitas para raspar datos a escala está bajo el capó,\n",
      "haciendo que tus raspadores web sean imparables. Prueba la API Scraping Browser de Bright Data gratis ahora con el enlace en la descripción.\n",
      "Pero la siguiente gran herramienta de IA que necesitas conocer es Stable Diffusion 3 Medium.\n",
      "Los pesos del modelo se acaban de liberar hace unas horas,\n",
      "y es el modelo de texto a imagen abierto más avanzado que existe.\n",
      "Lamentablemente, solo está disponible bajo una licencia no comercial, pero el nivel de calidad es bastante increíble y ahora puede generar texto de manera confiable a partir de tus indicaciones.\n",
      "Ahora, si actualmente tienes una novia de IA, te recomendaría encarecidamente actualizarla a este nuevo modelo.\n",
      "La personalidad es lo que importa, pero se verá bastante mediocre en comparación con todas tus amigas en ST3.\n",
      "Ahora, otra herramienta que realmente es útil es este generador de efectos de sonido de Eleven Labs,\n",
      "la misma compañía que diseñó mi voz. Todo lo que tienes que hacer es describir lo que quieres oír,\n",
      "y generará múltiples efectos de sonido. Aquí hay un ejemplo de dos resultados diferentes.\n",
      "Y el ejemplo dos. Ahora, lo que no te dije es que un ejemplo era real y otro fue generado por IA.\n",
      "Apuesto a que no puedes notar la diferencia, pero ahora necesitamos hablar sobre la generación de código.\n",
      "He estado esperando pacientemente para que la IA tome mi trabajo de programación,\n",
      "pero hasta ahora he estado decepcionado. Sin embargo, todavía puede haber esperanza.\n",
      "Hace unas semanas, la startup francesa Mistral lanzó un nuevo modelo llamado CodeStroll.\n",
      "Es también un modelo abierto, pero aún no se puede usar con fines comerciales y rinde extremadamente bien en puntos de referencia de codificación en comparación con otros modelos abiertos.\n",
      "He estado jugando con él en Ollama, y aunque es extremadamente impresionante y también muy rápido,\n",
      "todavía lucha con el problema antiguo de centrar un div. Ahora,\n",
      "cuando se trata de AI escribiendo código, hay dos tipos de personas.\n",
      "Están las personas como Devin que están maximizando la AI y tratando de hacer que la AI escriba casi el 100% de nuestro código.\n",
      "Estas personas suelen ser jóvenes e ingenuas, según los pesimistas de la IA en el otro extremo del espectro.\n",
      "Estos suelen ser boomers que piensan que el código de AI es totalmente basura y no tiene lugar en la industria.\n",
      "Pero la opinión óptima probablemente esté en algún punto intermedio. Otra herramienta gratuita que puedes usar ahora mismo es Cursor,\n",
      "que es una bifurcación de VS Code, y uno de los primeros editores de código verdaderamente enfocados en IA.\n",
      "En lugar de memorizar la sintaxis, dale el contexto de una base de código existente o documentación,\n",
      "luego presiona Ctrl-K y escribe tu código en lenguaje natural. Y si te preocupa que esté escribiendo código basura,\n",
      "puedes imponer ciertas reglas e incluso hacer que realice una revisión de código.\n",
      "Es como GitHub Copilot a lo grande. La IA generativa todavía tiene un largo camino por recorrer,\n",
      "pero si yo fuera Jada ahora mismo, estaría muy preocupado por la cantidad de progreso que han logrado estos nerds de la ciencia de datos en solo el último año.\n",
      "Esto ha sido The Code Report. Gracias por ver, y nos vemos en el próximo.\n",
      "### French Translation: Il y a un an, cette vidéo incroyable de Will Smith mangeant des spaghettis a fait sensation dans le monde entier.\n",
      "Les humains en plaisantions. Nous pouvions facilement dire que c'était faux,\n",
      "et à ce moment-là, personne n'avait vraiment peur. Mais avancez d'un an,\n",
      "et la technologie de l'IA générative a fait un saut énorme.\n",
      "Will Smith mangeant des spaghettis en 2024 n'est pas à prendre à la légère.\n",
      "Si cela ne se stabilise pas bientôt, cela pourrait mettre nos idoles d'Hollywood hors affaires,\n",
      "et il n'y aurait plus personne pour nous laver le cerveau.\n",
      "Dans la vidéo d'aujourd'hui, nous plongerons plus loin dans la vallée de l'étrange et examinerons cinq nouveaux outils d'IA générative que vous pouvez vraiment utiliser aujourd'hui.\n",
      "À la fin de cette vidéo, vous serez en mesure de licencier votre photographe humain,\n",
      "vidéaste, ingénieur du son et programmeur. Nous sommes le 17 juin 2024,\n",
      "et vous regardez The Code Report. Il y a quelques mois,\n",
      "OpenAI a présenté Sora et nous a alléchés avec une série de vidéos d'IA.\n",
      "Google a suivi avec Veeo, qui était également assez impressionnant, mais juste cette semaine,\n",
      "les Chinois ont lancé un nouveau modèle appelé Kling. Ils peuvent générer des vidéos de deux minutes de long,\n",
      "jusqu'à 30 FPS. C'est incroyablement impressionnant et probablement meilleur que Sora.\n",
      "Mais il y a un gros problème avec tous ces modèles.\n",
      "Ils ne sont pas disponibles pour le public. Heureusement, un nouvel outil vient de sortir appelé Dream Machine de LumaLabs,\n",
      "et il vous permet de créer des clips vidéo relativement réalistes.\n",
      "Je lui ai demandé de générer deux vieux hommes faisant du yoga,\n",
      "et le résultat est indiscernable de la vie réelle, à moins bien sûr de regarder de très près les doigts.\n",
      "De plus, c'est l'outil qui a été utilisé pour générer un Will Smith mangeant des spaghettis réaliste,\n",
      "et bien que ce soit impressionnant, il n'y a vraiment pas encore d'utilisation pratique ou commerciale pour cet outil.\n",
      "Tout ce qu'il fait vraiment bien est de simuler des cauchemars.\n",
      "Avant de passer au sujet suivant, parlons d'une chose dont vos LLM et modèles d'IA ne peuvent se passer,\n",
      "les données. La collecte de données sur le web était autrefois un cauchemar.\n",
      "Vous deviez configurer des réseaux proxy, développer des débloqueurs web pour contourner les CAPTCHA,\n",
      "gérer des erreurs de serveur, des empreintes digitales des navigateurs et toutes sortes d'autres problèmes.\n",
      "Mais avec des proxies résidentiels et des outils d'automatisation web comme Selenium,\n",
      "Puppeteer et Playwright, vous pouvez scraper le web à une échelle massive,\n",
      "sans exploser votre budget dans le processus. C'est là que Bright Data entre en jeu,\n",
      "le sponsor de la vidéo d'aujourd'hui. Ils portent vos opérations de scrape au niveau supérieur à une fraction du coût.\n",
      "Avec leur API Scraping Browser, vous pouvez oublier les proxies et les débloqueurs web.\n",
      "Tout ce dont vous avez besoin pour scraper des données à grande échelle est sous le capot,\n",
      "rendant vos scrapers web imparables. Essayez l'API Scraping Browser de Bright Data gratuitement maintenant avec le lien dans la description.\n",
      "Mais le prochain grand outil d'IA que vous devez connaître est Stable Diffusion 3 Medium.\n",
      "Les poids du modèle viennent d'être publiés il y a quelques heures,\n",
      "et c'est le modèle de texte à image le plus avancé disponible.\n",
      "Malheureusement, il n'est disponible que sous une licence non commerciale, mais le niveau de qualité est assez incroyable et peut désormais générer de manière fiable du texte à partir de vos invites.\n",
      "Maintenant, si vous avez actuellement une petite amie IA, je vous recommande vivement de la mettre à niveau vers ce nouveau modèle.\n",
      "C'est la personnalité qui compte, mais elle va paraître assez moyenne par rapport à toutes vos amies sur ST3.\n",
      "Maintenant, un autre outil qui est vraiment utile est ce générateur d'effets sonores de Eleven Labs,\n",
      "la même entreprise qui a conçu ma voix. Tout ce que vous avez à faire est de décrire ce que vous voulez entendre et il générera plusieurs effets sonores.\n",
      "Voici un exemple de deux résultats différents. Et exemple deux. Maintenant ce que je ne vous ai pas dit,\n",
      "c'est qu'un exemple était réel et l'autre était généré par l'IA.\n",
      "Je parie que vous ne pouvez pas sentir la différence, mais maintenant,\n",
      "nous devons parler de la génération de code. J'ai attendu patiemment que l'IA prenne mon travail de programmation,\n",
      "mais jusqu'à présent, j'ai été déçu. Cependant, il peut encore y avoir de l'espoir.\n",
      "Il y a quelques semaines, la startup française Mistral a lancé un nouveau modèle appelé CodeStroll.\n",
      "C'est aussi un modèle open source, mais il ne peut pas encore être utilisé à des fins commerciales,\n",
      "et il performe extrêmement bien sur les benchmarks de codage par rapport à d'autres modèles open source.\n",
      "J'ai joué avec sur Ollama, et bien qu'il soit extrêmement impressionnant et également très rapide,\n",
      "il a encore du mal avec le vieux problème de centrer un div.\n",
      "Maintenant, quand il s'agit de l'IA écrivant du code, il y a deux types de personnes.\n",
      "Il y a les gens comme Devin qui maximisent l'IA, essayant de faire écrire près de 100% de notre code par l'IA.\n",
      "Ces personnes sont généralement jeunes et naïves selon les pessimistes de l'IA à l'autre bout du spectre.\n",
      "Ce sont généralement des boomers qui pensent que le code IA est total slop et n'a pas sa place dans l'industrie.\n",
      "Mais l'avis optimal se situe probablement quelque part entre les deux.\n",
      "Un autre outil gratuit que vous pouvez utiliser dès maintenant est Cursor,\n",
      "qui est un fork de VS Code, et l'un des premiers éditeurs de code réellement axés sur l'IA.\n",
      "Au lieu de mémoriser la syntaxe, donnez-lui le contexte d'une base de code existante ou de la documentation,\n",
      "puis appuyez sur Ctrl-K et écrivez votre code avec le langage naturel.\n",
      "Et si vous craignez qu'il écrive du code pourri, vous pouvez imposer certaines règles et même lui faire effectuer une revue de code.\n",
      "C'est comme GitHub Copilot sous stéroïdes. L'IA générative a encore un long chemin à parcourir,\n",
      "mais si j'étais Jada en ce moment, je serais très préoccupé par la quantité de progrès réalisés par ces nerds de la science des données en seulement la dernière année.\n",
      "C'était The Code Report. Merci de regarder, et je vous verrai dans le prochain.\n",
      " > ===========================\n",
      " ˈɪŋlɪʃ tɹænzˈleɪʃən: wən jɪɹ əˈgoʊ, ðɪs ˌənbəˈlivəbəɫ ˈvɪdioʊ əv wɪɫ smɪθ ˈitɪŋ spəˈgɛti tʊk ðə wəɹɫd baɪ stɔɹm.\n",
      " length:113\n",
      " length:112\n",
      "wi ˈjumənz dʒoʊkt əˈbaʊt ɪt. wi kʊd ˈizəli tɛɫ ðət ɪt wɑz feɪk,\n",
      " length:63\n",
      " length:63\n",
      "ənd æt ðət pɔɪnt, ˈnoʊˌbɑˌdi wɑz ˈɹɪli əˈfɹeɪd. bət fæst ˈfɔɹwəɹd wən jɪɹ ˈleɪtəɹ,\n",
      " length:82\n",
      " length:82\n",
      "ənd ˈdʒɛnəɹətɪv eɪaɪ tɛk həz ˈteɪkən əˈnəðəɹ judʒ lip ˈfɔɹwəɹd. wɪɫ smɪθ ˈitɪŋ spəˈgɛti ɪn tˈwɛnti tˈwɛntiˌfɔɹ ɪz ˈnəθɪŋ tɪ dʒoʊk əɹaʊnd əˈbaʊt.\n",
      " length:144\n",
      " length:144\n",
      "ɪf ɪt ˈdəzənt plæˈtoʊ sun, ɪt kʊd pʊt ɑɹ ˈhɑliˌwʊd ˈaɪdəɫz aʊt əv ˈbɪznɪs,\n",
      " length:74\n",
      " length:74\n",
      "ənd ðɛɹ wʊd bi noʊ wən lɛft tɪ ˈbɹeɪnˌwɑʃ ˈjuˈɛs. ɪn ˈtudeɪz ˈvɪdioʊ,\n",
      " length:69\n",
      " length:69\n",
      "wɪɫ dɪˈsɛnd ˈfəɹðəɹ ˈɪntu ənˈkæni ˈvæli, ənd lʊk æt faɪv nu ˈdʒɛnəɹətɪv eɪaɪ tuɫz ðət ju kən ˈæˌktʃuəli juz təˈdeɪ.\n",
      " length:115\n",
      " length:115\n",
      "baɪ ðə ɛnd əv ðɪs ˈvɪdioʊ, juɫ bi ˈeɪbəɫ tɪ faɪəɹ jʊɹ ˈjumən fəˈtɑgɹəfəɹ,\n",
      " length:73\n",
      " length:73\n",
      "vɪdiˈɔgɹəfəɹ, saʊnd ˈɛndʒəˈnɪɹ, ənd ˈpɹoʊˌgɹæməɹ. ɪt ɪz dʒun ˈsɛvənˈtinθ, tˈwɛnti tˈwɛntiˌfɔɹ, ənd jʊɹ ˈwɑtʃɪŋ ðə koʊd ɹɪˈpɔɹt.\n",
      " length:127\n",
      " length:127\n",
      "ə fju mənθs əˈgoʊ, ˈoʊpən eɪaɪ ˈpɹivˌjud soɹa*, ənd tizd ˈjuˈɛs wɪθ ə bəntʃ əv eɪaɪ ˈvɪdioʊz.\n",
      " length:93\n",
      " length:93\n",
      "ˈgugəɫ ˈleɪtəɹ ˈfɑloʊd ðət əp wɪθ veeo*, wɪtʃ wɑz ˈɔlsoʊ kwaɪt ˌɪmˈpɹɛsɪv,\n",
      " length:74\n",
      " length:74\n",
      "bət dʒɪst ðɪs wik, ðə tʃaɪˈniz dɹɑpt ə nu ˈmɑdəɫ kɔɫd klɪŋ.\n",
      " length:59\n",
      " length:59\n",
      "ðeɪ kən ˈdʒɛnəɹˌeɪt ˈvɪdioʊz tu ˈmɪnəts lɔŋ, əp tɪ ˈθəɹˌdi fps*.\n",
      " length:64\n",
      " length:64\n",
      "ɪts ˌɪnˈkɹɛdəbli ˌɪmˈpɹɛsɪv, ənd ˈɑɹgjuəbli ˈbɛtəɹ ðən soɹa*. bət ðɛɹz wən bɪg ˈpɹɑbləm wɪθ ɔɫ əv ðiz ˈmɑdəɫz.\n",
      " length:110\n",
      " length:110\n",
      "ðɛɹ nɑt əˈveɪləbəɫ tɪ ðə ˈpəblɪk. wɛɫ ˈləkəli, ə nu tuɫ dʒɪst dɹɑpt kɔɫd ðə dɹim məˈʃin fɹəm ˈlumə læbz,\n",
      " length:104\n",
      " length:104\n",
      "ənd ɪt əˈlaʊz ju tɪ kɹiˈeɪt ˈɹɛlətɪvli ˌɹiəˈlɪstɪk ˈvɪdioʊ klɪps. aɪ ˈpɹɑmptɪd ɪt fəɹ tu oʊɫd mɛn duɪŋ ˈjoʊgə,\n",
      " length:110\n",
      " length:110\n",
      "ənd ðə ɹɪˈzəɫt ɪz ˌɪndɪˈstɪŋgwɪʃəbəɫ fɹəm ɹiɫ laɪf, ənˈlɛs əv kɔɹs ju lʊk ˈvɛɹi ˈkloʊsli æt ðə ˈfɪŋgəɹz.\n",
      " length:104\n",
      " length:104\n",
      "ɪn əˈdɪʃən, ðɪs ɪz ðə tuɫ juzd tɪ ˈdʒɛnəɹˌeɪt ə ˌɹiəˈlɪstɪk wɪɫ smɪθ ˈitɪŋ spəˈgɛti,\n",
      " length:84\n",
      " length:84\n",
      "ənd waɪɫ ˌɪmˈpɹɛsɪv, ðɛɹz ˈɹɪli noʊ ˈpɹæktɪkəɫ əɹ kəˈməɹʃəɫ juz fəɹ ðɪs tuɫ jɛt.\n",
      " length:80\n",
      " length:80\n",
      "ɔɫ ɪts ˈɹɪli gʊd æt ɪz ˈsɪmjəˌleɪtɪŋ ˈnaɪtˌmɛɹz. ˌbiˈfɔɹ wi muv ɔn tɪ ðə nɛkst ˈtɑpɪk ðoʊ,\n",
      " length:90\n",
      " length:90\n",
      "lɛts tɔk əˈbaʊt ˈsəmθɪŋ jʊɹ ɫlms* ənd eɪaɪ ˈmɑdəɫz kænt lɪv wɪˈθaʊt,\n",
      " length:68\n",
      " length:68\n",
      "ˈdætə. ˈdætə kəˈlɛkʃən ɔn ðə wɛb juzd tɪ bi ə ˈnaɪtˌmɛɹ.\n",
      " length:56\n",
      " length:56\n",
      "jʊd hæv tɪ sɛt əp ˈpɹɑksi ˈnɛtˌwəɹks, dɪˈvɛləp wɛb unblockeɹs* tɪ ˈbaɪˌpæs captchas*,\n",
      " length:85\n",
      " length:82\n",
      "diɫ wɪθ ˈsəɹvəɹ ˈɛɹəɹz, ˈbɹaʊzəɹ ˈfɪŋgəɹˌpɹɪntɪŋ, ənd ɔɫ sɔɹts əv ˈəðəɹ ˈɪʃuz.\n",
      " length:78\n",
      " length:78\n",
      "bət wɪθ ˌɹɛzɪˈdɛnʃəɫ ˈpɹɑksiz ənd wɛb ɔtəˈmeɪʃən tuɫz laɪk səˈliniəm, pəpəˈtiɹ,\n",
      " length:79\n",
      " length:79\n",
      "ənd ˈpleɪˌɹaɪt, ju kən skɹeɪp ðə wɛb ɔn ə ˈmæsɪv skeɪɫ,\n",
      " length:55\n",
      " length:55\n",
      "wɪˈθaʊt bloʊɪŋ əp jʊɹ ˈbədʒɪt ɪn ðə ˈpɹɔˌsɛs. ðæts wɛɹ bɹaɪt ˈdætə kəmz ɪn,\n",
      " length:75\n",
      " length:75\n",
      "ðə ˈspɑnsəɹ əv ˈtudeɪz ˈvɪdioʊ. ðeɪ teɪk jʊɹ skɹeɪp ɑps tɪ ðə nɛkst ˈlɛvəɫ æt ə ˈfɹækʃən əv ðə kɔst.\n",
      " length:100\n",
      " length:100\n",
      "wɪθ ðɛɹ ˈskɹeɪpɪŋ ˈbɹaʊzəɹ api*, ju kən fəɹˈgɛt əˈbaʊt ˈpɹɑksiz ənd wɛb unblockeɹs*.\n",
      " length:84\n",
      " length:83\n",
      "ˈɛvɹiˌθɪŋ ju nid tɪ skɹeɪp ˈdætə æt skeɪɫ ɪz ˈəndəɹ ðə hʊd,\n",
      " length:59\n",
      " length:59\n",
      "ˈmeɪkɪŋ jʊɹ wɛb ˈskɹeɪpəɹz ənˈstɑpəbəɫ. tɹaɪ bɹaɪt ˈdætəz ˈskɹeɪpɪŋ ˈbɹaʊzəɹ api* fəɹ fɹi ɹaɪt naʊ wɪθ ðə lɪŋk ɪn ðə dɪˈskɹɪpʃən.\n",
      " length:129\n",
      " length:129\n",
      "bət ðə nɛkst bɪg eɪaɪ tuɫ ju nid tɪ noʊ əˈbaʊt ɪz ˈsteɪbəɫ dɪfˈjuʒən θɹi ˈmidiəm.\n",
      " length:81\n",
      " length:81\n",
      "ðə ˈmɑdəɫ weɪts wəɹ dʒɪst ɹiˈlist aʊəɹz əˈgoʊ, ənd ɪts ðə moʊst ədˈvænst ˈoʊpən tɛkst tɪ ˈɪmɪdʒ ˈmɑdəɫ aʊt ðɛɹ.\n",
      " length:111\n",
      " length:111\n",
      "ənˈfɔɹtʃənətli, ɪts ˈoʊnli əˈveɪləbəɫ ˈəndəɹ ə ˈnɑnkəˈməɹʃəɫ ˈlaɪsəns, bət ðə ˈlɛvəɫ əv kˈwɑləti ɪz ˈpɹɪti əˈmeɪzɪŋ,\n",
      " length:116\n",
      " length:116\n",
      "ənd kən naʊ ɹɪˈlaɪəbli ˈdʒɛnəɹˌeɪt tɛkst fɹəm jʊɹ pɹɑmpts. naʊ ɪf ju ˈkəɹəntli hæv ən eɪaɪ ˈgəɹlˌfɹɛnd,\n",
      " length:103\n",
      " length:103\n",
      "aɪ wʊd ˈhaɪli ˌɹɛkəˈmɛnd ˈəpˌgɹeɪdɪŋ həɹ tɪ ðɪs nu ˈmɑdəɫ. ɪts ðə ˌpəɹsəˈnælɪti ðət ˈmætəɹz,\n",
      " length:92\n",
      " length:92\n",
      "bət ʃiz goʊɪŋ tɪ bi ˈlʊkɪŋ ˈpɹɪti mɪd kəmˈpɛɹd tɪ ɔɫ jʊɹ fɹɛndz ɔn stthɹee*.\n",
      " length:76\n",
      " length:76\n",
      "naʊ əˈnəðəɹ tuɫ ðæts ˈæˌktʃuəli ˈjusfəɫ ɪz ðɪs saʊnd ˈifɛkt ˈdʒɛnəɹˌeɪtəɹ fɹəm ˈilɛvən læbz,\n",
      " length:92\n",
      " length:92\n",
      "ðə seɪm ˈkəmpəˌni ðət ˌɛndʒəˈniɹd maɪ vɔɪs. ɔɫ ju hæv tɪ du ɪz dɪˈskɹaɪb wət ju wɔnt tɪ hiɹ,\n",
      " length:92\n",
      " length:92\n",
      "ənd ɪt wɪɫ ˈdʒɛnəɹˌeɪt ˈməltəpəɫ saʊnd ˈifɛkts. hɪɹz ən ɪgˈzæmpəɫ əv tu ˈdɪfəɹənt ɹɪˈzəɫts.\n",
      " length:91\n",
      " length:91\n",
      "ənd ɪgˈzæmpəɫ tu. naʊ wət aɪ feɪɫd tɪ tɛɫ ju ɪz ðət wən ɪgˈzæmpəɫ wɑz ɹiɫ,\n",
      " length:74\n",
      " length:74\n",
      "ənd wən wɑz eɪaɪ ˈdʒɛnəɹˌeɪtɪd. aɪ bɛt ju kænt smɛɫ ðə ˈdɪfəɹəns,\n",
      " length:65\n",
      " length:65\n",
      "bət naʊ wi nid tɪ tɔk əˈbaʊt koʊd ˌdʒɛnəɹˈeɪʃən. aɪv bɪn ˈpeɪʃəntli ˈweɪtɪŋ fəɹ eɪaɪ tɪ teɪk maɪ ˈpɹoʊˌgɹæmɪŋ dʒɑb,\n",
      " length:115\n",
      " length:115\n",
      "bət soʊ fɑɹ aɪv bɪn ˌdɪsəˈpɔɪnɪd. ˌhaʊˈɛvəɹ, ðɛɹ meɪ stɪɫ bi hoʊp.\n",
      " length:66\n",
      " length:66\n",
      "ə fju wiks əˈgoʊ, ðə fɹɛntʃ ˈstɑɹˌtəp ˈmɪstɹəɫ ɹiˈlist ə nu ˈmɑdəɫ kɔɫd koʊd stɹoʊɫ.\n",
      " length:84\n",
      " length:84\n",
      "ɪts ˈɔlsoʊ ən ˈoʊpən ˈmɑdəɫ, bət ˈkænɑt bi juzd fəɹ kəˈməɹʃəɫ ˈpəɹpəsɪz jɛt,\n",
      " length:76\n",
      " length:76\n",
      "ənd pəɹˈfɔɹmz ɪkˈstɹimli wɛɫ ɔn ˈkoʊdɪŋ ˈbɛntʃˌmɑɹks kəmˈpɛɹd tɪ ˈəðəɹ ˈoʊpən ˈmɑdəɫz.\n",
      " length:86\n",
      " length:86\n",
      "aɪv bɪn pleɪɪŋ əɹaʊnd wɪθ ɪt ɔn ollama*, ənd waɪɫ ɪts ɪkˈstɹimli ˌɪmˈpɹɛsɪv,\n",
      " length:76\n",
      " length:76\n",
      "ənd ˈɔlsoʊ ˈvɛɹi fæst, ɪt stɪɫ ˈstɹəgəɫz wɪθ ðə eɪdʒ oʊɫd ˈpɹɑbləm əv ˈsɛntəɹɪŋ ə div*.\n",
      " length:87\n",
      " length:87\n",
      "naʊ wɪn ɪt kəmz tɪ eɪaɪ ˈɹaɪtɪŋ koʊd, ðɛɹ əɹ tu taɪps əv ˈpipəɫ.\n",
      " length:64\n",
      " length:64\n",
      "ðɛɹz ˈpipəɫ laɪk ˈdɛvɪn hu əɹ duɪŋ eɪaɪ maxing*, ənd tɹaɪɪŋ tɪ gɪt eɪaɪ tɪ ɹaɪt ˈnɪɹli wən ˈhənəɹd% əv ɑɹ koʊd.\n",
      " length:111\n",
      " length:110\n",
      "ðiz ˈpipəɫ əɹ ˈjuʒəwəli jəŋ ənd naɪiv, əˈkɔɹdɪŋ tɪ ðə eɪaɪ doomeɹs* ɔn ðə ˈəðəɹ ɛnd əv ðə ˈspɛktɹəm.\n",
      " length:100\n",
      " length:100\n",
      "ðɛɹ ˈjuʒəwəli ˈbuməɹz hu θɪŋk ðət eɪaɪ koʊd ɪz ˈtoʊtəɫ slɑp,\n",
      " length:60\n",
      " length:60\n",
      "ənd həz noʊ pleɪs ɪn ðə ˈɪndəstɹi. bət ðə ˈɑptɪməɫ hɑt teɪk ɪz ˈlaɪkli ˈsəmˌwɛɹ ɪn bɪtˈwin.\n",
      " length:91\n",
      " length:91\n",
      "əˈnəðəɹ fɹi tuɫ ju kən juz ɹaɪt naʊ ɪz ˈkəɹsəɹ, wɪtʃ ɪz ə fɔɹk əv ˈvəɹsəz koʊd,\n",
      " length:79\n",
      " length:79\n",
      "ənd wən əv ðə fəɹst ˈtɹuli eɪaɪ ˈfoʊkɪst koʊd ˈɛdɪtəɹz. ˌɪnˈstɛd əv ˈmɛməɹˌaɪzɪŋ ˈsɪnˌtæks,\n",
      " length:91\n",
      " length:91\n",
      "gɪv ɪt ðə ˈkɑntɛkst əv ən ɪgˈzɪstɪŋ koʊd beɪs əɹ ˌdɑkjəmɛnˈteɪʃən,\n",
      " length:66\n",
      " length:66\n",
      "ðɛn hɪt ctɹɫ-k* ənd ɹaɪt jʊɹ koʊd wɪθ ˈnætʃəɹəɫ ˈlæŋgwɪdʒ. ənd ɪf jʊɹ ˈwəɹid ðət ɪts ˈɹaɪtɪŋ ˈgɑɹbɪdʒ koʊd,\n",
      " length:107\n",
      " length:106\n",
      "ju kən ɛnˈfɔɹs ˈsəɹtən ɹuɫz ənd ˈivɪn hæv ɪt pəɹˈfɔɹm ə koʊd ˌɹivˈju.\n",
      " length:69\n",
      " length:69\n",
      "ɪts laɪk git* həb ˈkoʊpaɪlət ɔn ˈstɛɹɔɪdz. ˈdʒɛnəɹətɪv eɪaɪ stɪɫ həz ə lɔŋ weɪ tɪ goʊ,\n",
      " length:86\n",
      " length:86\n",
      "bət ɪf aɪ wəɹ jada* ɹaɪt naʊ, aɪd bi ˈvɛɹi kənˈsəɹnd əˈbaʊt ðə əˈmaʊnt əv ˈpɹɑˌgɹɛs meɪd baɪ ðiz ˈdætə saɪəns nəɹdz ɪn dʒɪst ðə læst jɪɹ.\n",
      " length:137\n",
      " length:137\n",
      "ðɪs həz bɪn ðə koʊd ɹɪˈpɔɹt. θæŋks fəɹ ˈwɑtʃɪŋ, ənd aɪ wɪɫ si ju ɪn ðə nɛkst wən.\n",
      " length:81\n",
      " length:81\n",
      " ˈspænɪʃ tɹænzˈleɪʃən: hace* ˈjuˈɛn ˈænoʊ, ˈɛsteɪ incɹeible* ˈvɪdioʊ də wɪɫ smɪθ comiendo* espaguetis* soɹpɹendio* æɫ ˈməndoʊ.\n",
      " length:126\n",
      " length:122\n",
      "lɔs humanos* bɹomeabamos* æɫ ɹespecto*. podiamos* deciɹ* facilmente* kju ˈɪɹə falso*,\n",
      " length:85\n",
      " length:82\n",
      "waɪ ɛn ese* momento*, nadie* estaba* ɹealmente* asustado*. pəɹoʊ avanzando* ˈjuˈɛn ˈænoʊ,\n",
      " length:89\n",
      " length:89\n",
      "lɑ tecnologia* də ˈiə geneɹativa* hɑ ˈdeɪˌdoʊ otɹo* gɹæn salto* adelante*.\n",
      " length:74\n",
      " length:73\n",
      "wɪɫ smɪθ comiendo* espaguetis* ɛn tˈwɛnti tˈwɛntiˌfɔɹ noʊ ɛs motivo* də bɹoma*.\n",
      " length:79\n",
      " length:78\n",
      "si noʊ seɪ estabiliza* ˈpɹɑntoʊ, podɹia* poneɹ* ə nuestɹos* idolos* də ˈhɑliˌwʊd fueɹa* dɛɫ negocio*,\n",
      " length:101\n",
      " length:100\n",
      "waɪ noʊ quedaɹia* nadie* ˈpɛɹə lavaɹnos* ɛɫ ceɹebɹo*. ɛn ɛɫ ˈvɪdioʊ də hɔɪ,\n",
      " length:75\n",
      " length:73\n",
      "descendeɹemos* mɑz ɛn ɛɫ veɪɫ inquietante*, waɪ veɹemos* ˈsɪŋkoʊ nuevas* heɹɹamientas* də ˈiə geneɹativa* kju puedes* usaɹ* hɔɪ ɛn ˈdiə.\n",
      " length:136\n",
      " length:134\n",
      "æɫ ˈfaɪnəɫ də ˈɛsteɪ ˈvɪdioʊ, podɹas* despediɹ* ə tu fotogɹafo* humano*,\n",
      " length:72\n",
      " length:72\n",
      "videogɹafo*, ingenieɹo* də sonido* waɪ pɹogɹamadoɹ*. ɛs ɛɫ ˈsɛvənˈtin də junio* də tˈwɛnti tˈwɛntiˌfɔɹ,\n",
      " length:103\n",
      " length:103\n",
      "waɪ estas* viendo* ðə koʊd ɹɪˈpɔɹt. hace* ˈunoʊz meses*, ˈoʊpən eɪaɪ pɹesento* soɹa* waɪ nos* intɹigo* kɑn ˈunə seɹie* də ˈvɪdioʊz də ˈiə.\n",
      " length:138\n",
      " length:137\n",
      "ˈgugəɫ despues* siguio* kɑn veeo*, kju tambien* fue* bastante* impɹesionante*, pəɹoʊ ˈdʒəstoʊ ˈɛstə semana*,\n",
      " length:108\n",
      " length:108\n",
      "lɔs chinos* lanzaɹon* ˈjuˈɛn nˈweɪvoʊ moʊˈdɛloʊ llamado* klɪŋ. pueden* geneɹaɹ* ˈvɪdioʊz də dɔs minutos* də duɹacion*,\n",
      " length:118\n",
      " length:116\n",
      "ˈɑstə ˈθəɹˌdi fps*. ɛs incɹeiblemente* impɹesionante* waɪ, posiblemente*, mejoɹ* kju soɹa*.\n",
      " length:91\n",
      " length:90\n",
      "pəɹoʊ heɪ ˈjuˈɛn gɹæn pɹoblema* kɑn ˌtuˈduz estos* modelos*. noʊ ˈɛstən disponibles* ˈpɛɹə ɛɫ publico*.\n",
      " length:103\n",
      " length:102\n",
      "pues* afoɹtunadamente*, acaba* də saliɹ* ˈunə nueva* heɹɹamienta* llamada* dɹim məˈʃin də ˈlumə læbz,\n",
      " length:101\n",
      " length:100\n",
      "kju ti peɹmite* kɹɪɹ klɪps də ˈvɪdioʊ ɹelativamente* ɹealistas*. lə pedi* kju geneɹaɹa* dɔs hombɹes* mayoɹes* haciendo* ˈjoʊgə,\n",
      " length:127\n",
      " length:126\n",
      "waɪ ɛɫ ɹesultado* ɛs indistinguible* də lɑ ˈvaɪdə ɹiɫ, ə menos* kju,\n",
      " length:68\n",
      " length:68\n",
      "pɔɹ supuesto*, maɪɹz muy* də ceɹca* lɔs dedos*. ademas*, ˈɛstə ɛs lɑ heɹɹamienta* utilizada* ˈpɛɹə geneɹaɹ* ˈjuˈɛn wɪɫ smɪθ comiendo* espaguetis* ɹealista*,\n",
      " length:156\n",
      " length:153\n",
      "waɪ aunque* ɛs impɹesionante*, ɹealmente* noʊ tiene* ˈjuˈɛn uso* pɹactico* oʊ comeɹciaɫ* todavia*.\n",
      " length:98\n",
      " length:93\n",
      "ˌtuˈdu loʊ kju ɹealmente* sabe* haceɹ* ɛs simulaɹ* pesadillas*. ˈæntiz də pasaɹ* æɫ siguiente* tema*,\n",
      " length:101\n",
      " length:100\n",
      "hablemos* də algo* də loʊ kju tus* ɫlms* waɪ modelos* də ˈiə noʊ pueden* pɹescindiɹ*,\n",
      " length:85\n",
      " length:84\n",
      "datos*. lɑ ɹecopilacion* də datos* ɛn lɑ wɛb solia* seɹ* ˈunə pesadilla*.\n",
      " length:73\n",
      " length:71\n",
      "tenias* kju configuɹaɹ* ɹedes* də ˈpɹɑksi, desaɹɹollaɹ* desbloqueadoɹes* wɛb ˈpɛɹə soɹteaɹ* captchas*,\n",
      " length:102\n",
      " length:98\n",
      "lidiaɹ* kɑn eɹɹoɹes* də seɹvidoɹ*, huellas* digitales* də navegadoɹ* waɪ ˌtuˈdu ˈtipoʊ də otɹos* pɹoblemas*.\n",
      " length:108\n",
      " length:108\n",
      "pəɹoʊ kɑn ˈpɹɑksiz ɹesidenciales* waɪ heɹɹamientas* də automatizacion* wɛb ˈkoʊmoʊ səˈliniəm,\n",
      " length:93\n",
      " length:91\n",
      "pəpəˈtiɹ waɪ ˈpleɪˌɹaɪt, puedes* ɹastɹeaɹ* lɑ wɛb ə ˈunə escala* masiva*,\n",
      " length:73\n",
      " length:72\n",
      "sɪn consumiɹ* tu pɹesupuesto* ɛn ɛɫ ˌpɹoʊˈsɛsoʊ. ahi* ɛs dɑnd entɹa* bɹaɪt ˈdætə,\n",
      " length:81\n",
      " length:80\n",
      "ɛɫ patɹocinadoɹ* dɛɫ ˈvɪdioʊ də hɔɪ. llevan* tus* opeɹaciones* də ɹastɹeo* æɫ siguiente* niveɫ* ə ˈunə fɹaccion* dɛɫ costo*.\n",
      " length:124\n",
      " length:119\n",
      "kɑn su api* ˈskɹeɪpɪŋ ˈbɹaʊzəɹ, puedes* olvidaɹte* də lɔs ˈpɹɑksiz waɪ desbloqueadoɹes* wɛb.\n",
      " length:92\n",
      " length:91\n",
      "ˌtuˈdu loʊ kju necesitas* ˈpɛɹə ɹaspaɹ* datos* ə escala* ˈɛstə bajo* ɛɫ ˈkɑpoʊ,\n",
      " length:79\n",
      " length:77\n",
      "haciendo* kju tus* ɹaspadoɹes* wɛb ʃɔn impaɹables*. pɹueba* lɑ api* ˈskɹeɪpɪŋ ˈbɹaʊzəɹ də bɹaɪt ˈdætə ˈgɹætəs ahoɹa* kɑn ɛɫ enlace* ɛn lɑ descɹipcion*.\n",
      " length:151\n",
      " length:147\n",
      "pəɹoʊ lɑ siguiente* gɹæn heɹɹamienta* də ˈiə kju necesitas* conoceɹ* ɛs ˈsteɪbəɫ dɪfˈjuʒən θɹi ˈmidiəm.\n",
      " length:103\n",
      " length:100\n",
      "lɔs ˈpeɪsoʊz dɛɫ moʊˈdɛloʊ seɪ acaban* də libeɹaɹ* hace* unas* hoɹas*,\n",
      " length:70\n",
      " length:68\n",
      "waɪ ɛs ɛɫ moʊˈdɛloʊ də texto* ə imagen* abieɹto* mɑz avanzado* kju existe*.\n",
      " length:75\n",
      " length:75\n",
      "lamentablemente*, ˈsoʊˌloʊ ˈɛstə disponible* bajo* ˈunə licencia* noʊ comeɹciaɫ*, pəɹoʊ ɛɫ niveɫ* də calidad* ɛs bastante* incɹeible* waɪ ahoɹa* puede* geneɹaɹ* texto* də maneɹa* confiable* ə paɹtiɹ* də tus* indicaciones*.\n",
      " length:222\n",
      " length:213\n",
      "ahoɹa*, si actualmente* tienes* ˈunə ˈnoʊviə də ˈiə, ti ɹecomendaɹia* encaɹecidamente* actualizaɹla* ə ˈɛsteɪ nˈweɪvoʊ moʊˈdɛloʊ.\n",
      " length:129\n",
      " length:124\n",
      "lɑ peɹsonalidad* ɛs loʊ kju impoɹta*, pəɹoʊ seɪ ˈvɛɹə bastante* ˌmidiˈoʊkəɹ ɛn compaɹacion* kɑn todas* tus* amigas* ɛn stthɹee*.\n",
      " length:128\n",
      " length:126\n",
      "ahoɹa*, otɹa* heɹɹamienta* kju ɹealmente* ɛs utiɫ* ɛs ˈɛsteɪ geneɹadoɹ* də efectos* də sonido* də ˈilɛvən læbz,\n",
      " length:111\n",
      " length:110\n",
      "lɑ misma* kəmˈpeɪˌniɑ kju diseno* mi voz*. ˌtuˈdu loʊ kju tienes* kju haceɹ* ɛs descɹibiɹ* loʊ kju quieɹes* oiɹ*,\n",
      " length:113\n",
      " length:110\n",
      "waɪ geneɹaɹa* ˈməltəpəɫz efectos* də sonido*. aqui* heɪ ˈjuˈɛn ejemplo* də dɔs ɹesultados* difeɹentes*.\n",
      " length:103\n",
      " length:101\n",
      "waɪ ɛɫ ejemplo* dɔs. ahoɹa*, loʊ kju noʊ ti dije* ɛs kju ˈjuˈɛn ejemplo* ˈɪɹə ɹiɫ waɪ otɹo* fue* geneɹado* pɔɹ ˈiə.\n",
      " length:115\n",
      " length:115\n",
      "apuesto* ə kju noʊ puedes* notaɹ* lɑ difeɹencia*, pəɹoʊ ahoɹa* necesitamos* hablaɹ* sobɹe* lɑ geneɹacion* də codigo*.\n",
      " length:117\n",
      " length:113\n",
      "hi estado* espeɹando* pacientemente* ˈpɛɹə kju lɑ ˈiə toʊm mi tɹabajo* də pɹogɹamacion*,\n",
      " length:88\n",
      " length:86\n",
      "pəɹoʊ ˈɑstə ahoɹa* hi estado* decepcionado*. sɪn ɛmˈbɑɹgoʊ, todavia* puede* ˈheɪbəɹ espeɹanza*.\n",
      " length:95\n",
      " length:93\n",
      "hace* unas* semanas*, lɑ ˈstɑɹˌtəp fɹancesa* ˈmɪstɹəɫ ˈlænzoʊ ˈjuˈɛn nˈweɪvoʊ moʊˈdɛloʊ llamado* koʊd stɹoʊɫ.\n",
      " length:109\n",
      " length:107\n",
      "ɛs tambien* ˈjuˈɛn moʊˈdɛloʊ abieɹto*, pəɹoʊ aun* noʊ seɪ puede* usaɹ* kɑn faɪnz comeɹciales* waɪ ɹinde* extɹemadamente* bin ɛn puntos* də ɹefeɹencia* də codificacion* ɛn compaɹacion* kɑn otɹos* modelos* abieɹtos*.\n",
      " length:214\n",
      " length:206\n",
      "hi estado* jugando* kɑn ɛɫ ɛn ollama*, waɪ aunque* ɛs extɹemadamente* impɹesionante* waɪ tambien* muy* ɹapido*,\n",
      " length:111\n",
      " length:110\n",
      "todavia* lucha* kɑn ɛɫ pɹoblema* antiguo* də centɹaɹ* ˈjuˈɛn div*. ahoɹa*,\n",
      " length:74\n",
      " length:72\n",
      "cuando* seɪ tɹata* də eɪaɪ escɹibiendo* codigo*, heɪ dɔs tipos* də peɹsonas*.\n",
      " length:77\n",
      " length:74\n",
      "ˈɛstən ˈɛˈleɪˈɛs peɹsonas* ˈkoʊmoʊ ˈdɛvɪn kju ˈɛstən maximizando* lɑ eɪaɪ waɪ tɹatando* də haceɹ* kju lɑ eɪaɪ escɹiba* casi* ɛɫ wən ˈhənəɹd% də nuestɹo* codigo*.\n",
      " length:161\n",
      " length:156\n",
      "estas* peɹsonas* suelen* seɹ* jovenes* i ingenuas*, segun* lɔs pesimistas* də lɑ ˈiə ɛn ɛɫ otɹo* extɹemo* dɛɫ espectɹo*.\n",
      " length:120\n",
      " length:119\n",
      "estos* suelen* seɹ* ˈbuməɹz kju piensan* kju ɛɫ codigo* də eɪaɪ ɛs totalmente* basuɹa* waɪ noʊ tiene* ˈlugəɹ ɛn lɑ ˌɪnˈdəstɹiə.\n",
      " length:127\n",
      " length:126\n",
      "pəɹoʊ lɑ əˈpɪnjən ˈɑptɪmə pɹobablemente* ˈɛsteɪ ɛn algun* ˈpunˌtoʊ inteɹmedio*. otɹa* heɹɹamienta* gɹatuita* kju puedes* usaɹ* ahoɹa* mismo* ɛs ˈkəɹsəɹ,\n",
      " length:152\n",
      " length:152\n",
      "kju ɛs ˈunə bifuɹcacion* də ˈvəɹsəz koʊd, waɪ ˈuˌnoʊ də lɔs pɹimeɹos* editoɹes* də codigo* veɹdadeɹamente* enfocados* ɛn ˈiə.\n",
      " length:125\n",
      " length:121\n",
      "ɛn ˈlugəɹ də memoɹizaɹ* lɑ sintaxis*, deɪɫ ɛɫ contexto* də ˈunə beɪs də codigo* existente* oʊ documentacion*,\n",
      " length:109\n",
      " length:105\n",
      "luego* pɹesiona* ctɹɫ-k* waɪ escɹibe* tu codigo* ɛn lenguaje* ˈnætʃəɹəɫ. waɪ si ti pɹeocupa* kju ˈɛsteɪ escɹibiendo* codigo* basuɹa*,\n",
      " length:133\n",
      " length:127\n",
      "puedes* imponeɹ* cieɹtas* ɹeglas* i incluso* haceɹ* kju ɹealice* ˈunə ɹiˈvɪʒən də codigo*.\n",
      " length:90\n",
      " length:85\n",
      "ɛs ˈkoʊmoʊ git* həb ˈkoʊpaɪlət ə loʊ gɹænd. lɑ ˈiə geneɹativa* todavia* tiene* ˈjuˈɛn ˈlɑɹgoʊ kəˈminoʊ pɔɹ ɹecoɹɹeɹ*,\n",
      " length:117\n",
      " length:116\n",
      "pəɹoʊ si joʊ fueɹa* jada* ahoɹa* mismo*, estaɹia* muy* pɹeocupado* pɔɹ lɑ cantidad* də pɹogɹeso* kju hɑn logɹado* estos* nəɹdz də lɑ ciencia* də datos* ɛn ˈsoʊˌloʊ ɛɫ ultimo* ˈænoʊ.\n",
      " length:181\n",
      " length:177\n",
      "esto* hɑ sido* ðə koʊd ɹɪˈpɔɹt. ˈgɹɑsiəz pɔɹ vəɹ, waɪ nos* vemos* ɛn ɛɫ pɹoximo*.\n",
      " length:81\n",
      " length:81\n",
      " fɹɛntʃ tɹænzˈleɪʃən: ɪɫ waɪ ə ˈjuˈɛn ən, cette* ˈvɪdioʊ incɹoyable* də wɪɫ smɪθ mangeant* dɪ spaghettis* ə feɪt sɛnˈseɪʃən dans* lə mɑnd entieɹ*.\n",
      " length:146\n",
      " length:143\n",
      "lɛs humains* ɛn plaisantions*. nus pouvions* facilement* daɪəɹ kju c'etait* foʊ,\n",
      " length:80\n",
      " length:77\n",
      "ɛt ə ce* moment-la*, peɹsonne* n'avait* vɹaiment* peuɹ*. meɪs avancez* d'un* ən,\n",
      " length:80\n",
      " length:76\n",
      "ɛt lɑ technologie* də l'ia* ˈdʒɛnəɹətɪv ə feɪt ˈjuˈɛn saut* enoɹme*.\n",
      " length:68\n",
      " length:66\n",
      "wɪɫ smɪθ mangeant* dɪ spaghettis* ɛn tˈwɛnti tˈwɛntiˌfɔɹ n'est* pɑz ə pɹendɹe* ə lɑ ˈlɛgəɹ.\n",
      " length:91\n",
      " length:90\n",
      "si ˈsɛlə ni seɪ stabilise* pɑz bientot*, ˈsɛlə pouɹɹait* mettɹe* nos* idoles* d'hollywood* ɔɹ affaiɹes*,\n",
      " length:104\n",
      " length:103\n",
      "ɛt ɪɫ n'y* auɹait* pləs peɹsonne* pɔɹ nus ˈleɪvəɹ lə ceɹveau*.\n",
      " length:62\n",
      " length:60\n",
      "dans* lɑ ˈvɪdioʊ d'aujouɹd'hui*, nus plongeɹons* pləs lɔɪn dans* lɑ ˈvæli də l'etɹange* ɛt examineɹons* sɪŋk nouveaux* outiɫs* d'ia* ˈdʒɛnəɹətɪv kju vu pouvez* vɹaiment* utiliseɹ* ˈoʊʒɔɹdˈwi.\n",
      " length:191\n",
      " length:187\n",
      "ə lɑ fɪn də cette* ˈvɪdioʊ, vu seɹez* ɛn mesuɹe* də licencieɹ* votɹe* photogɹaphe* humain*,\n",
      " length:91\n",
      " length:88\n",
      "videaste*, ingenieuɹ* də sən ɛt pɹogɹammeuɹ*. nus sommes* lə ˈsɛvənˈtin juin* tˈwɛnti tˈwɛntiˌfɔɹ,\n",
      " length:98\n",
      " length:98\n",
      "ɛt vu ɹegaɹdez* ðə koʊd ɹɪˈpɔɹt. ɪɫ waɪ ə quelques* mois*,\n",
      " length:58\n",
      " length:56\n",
      "ˈoʊpən eɪaɪ ə pɹesente* soɹa* ɛt nus ə alleches* avec* une* seɹie* də ˈvɪdioʊz d'ia*.\n",
      " length:85\n",
      " length:82\n",
      "ˈgugəɫ ə suivi* avec* veeo*, ki etait* egalement* assez* impɹessionnant*, meɪs juste* cette* semaine*,\n",
      " length:102\n",
      " length:100\n",
      "lɛs chinois* ont* læns ˈjuˈɛn ˌnuˈvoʊ modele* appele* klɪŋ. iɫs* peuvent* geneɹeɹ* dɪ ˈvɪdioʊz də du ˈmɪnəts də lɔŋ,\n",
      " length:116\n",
      " length:115\n",
      "jusqu'a* ˈθəɹˌdi fps*. sɛst incɹoyablement* impɹessionnant* ɛt pɹobablement* meilleuɹ* kju soɹa*.\n",
      " length:97\n",
      " length:94\n",
      "meɪs ɪɫ waɪ ə ˈjuˈɛn gɹoʊs pɹobleme* avec* tous* ces* modeles*.\n",
      " length:63\n",
      " length:61\n",
      "iɫs* ni sont* pɑz disponibles* pɔɹ lə ˈpəblɪk. heuɹeusement*, ˈjuˈɛn ˈnuvəɫ outiɫ* vient* də soɹtiɹ* appele* dɹim məˈʃin də ˈlumə læbz,\n",
      " length:135\n",
      " length:135\n",
      "ɛt ɪɫ vu peɹmet* də kɹɪɹ dɪ klɪps ˈvɪdioʊ ɹelativement* ɹealistes*.\n",
      " length:67\n",
      " length:67\n",
      "dʒi luɪ eɪaɪ demande* də geneɹeɹ* du vieux* hɑmz faisant* də ˈjoʊgə,\n",
      " length:68\n",
      " length:68\n",
      "ɛt lə ɹesultat* ɛst indisceɹnable* də lɑ vaɪ ɹeelle*, ə moins* bin səɹ də ɹegaɹdeɹ* də tɹes* pɹɛz lɛs doigts*.\n",
      " length:110\n",
      " length:109\n",
      "də pləs, sɛst l'outiɫ* ki ə ete* utilise* pɔɹ geneɹeɹ* ˈjuˈɛn wɪɫ smɪθ mangeant* dɪ spaghettis* ɹealiste*,\n",
      " length:106\n",
      " length:105\n",
      "ɛt bin kju ce* soit* impɹessionnant*, ɪɫ n'y* ə vɹaiment* pɑz ˈɑnˌkɔɹ d'utilisation* pɹatique* u kəˌməɹsiˈæɫ pɔɹ cet* outiɫ*.\n",
      " length:125\n",
      " length:120\n",
      "taʊt ce* qu'iɫ* feɪt vɹaiment* bin ɛst də simuleɹ* dɪ cauchemaɹs*.\n",
      " length:66\n",
      " length:61\n",
      "əˈvɑnt də ˈpæsəɹ oʊ sujet* suivant*, paɹlons* d'une* tʃoʊz dont* vɑs ɫlm* ɛt modeles* d'ia* ni peuvent* seɪ ˈpæsəɹ,\n",
      " length:115\n",
      " length:113\n",
      "lɛs donnees*. lɑ collecte* də donnees* səɹ lə wɛb etait* autɹefois* ˈjuˈɛn cauchemaɹ*.\n",
      " length:86\n",
      " length:82\n",
      "vu deviez* configuɹeɹ* dɪ ɹeseaux* ˈpɹɑksi, developpeɹ* dɪ debloqueuɹs* wɛb pɔɹ contouɹneɹ* lɛs captcha*,\n",
      " length:105\n",
      " length:100\n",
      "geɹeɹ* dɪ eɹɹeuɹs* də seɹveuɹ*, dɪ empɹeintes* digitales* dɪ navigateuɹs* ɛt toutes* soɹtes* d'autɹes* pɹoblemes*.\n",
      " length:114\n",
      " length:113\n",
      "meɪs avec* dɪ ˈpɹɑksiz ɹesidentieɫs* ɛt dɪ outiɫs* d'automatisation* wɛb comme* səˈliniəm,\n",
      " length:90\n",
      " length:87\n",
      "pəpəˈtiɹ ɛt ˈpleɪˌɹaɪt, vu pouvez* ˈskɹeɪpəɹ lə wɛb ə une* echelle* ˈmæsɪv,\n",
      " length:75\n",
      " length:74\n",
      "sænz exploseɹ* votɹe* ˈbədʒɪt dans* lə pɹocessus*. sɛst lɑ kju bɹaɪt ˈdætə ˈɑntɹə ɛn jeu*,\n",
      " length:90\n",
      " length:89\n",
      "lə ˈspɑnsəɹ də lɑ ˈvɪdioʊ d'aujouɹd'hui*. iɫs* ˈpɔɹtɛnt vɑs ˌɑpəɹˈeɪʃənz də skɹeɪp oʊ niveau* supeɹieuɹ* ə une* ˈfɹækʃən də cout*.\n",
      " length:130\n",
      " length:127\n",
      "avec* leuɹ* api* ˈskɹeɪpɪŋ ˈbɹaʊzəɹ, vu pouvez* oublieɹ* lɛs ˈpɹɑksiz ɛt lɛs debloqueuɹs* wɛb.\n",
      " length:94\n",
      " length:92\n",
      "taʊt ce* dont* vu avez* besoin* pɔɹ ˈskɹeɪpəɹ dɪ donnees* ə gɹænd echelle* ɛst suz lə capot*,\n",
      " length:93\n",
      " length:90\n",
      "ɹendant* vɑs ˈskɹeɪpəɹz wɛb impaɹables*. essayez* l'api* ˈskɹeɪpɪŋ ˈbɹaʊzəɹ də bɹaɪt ˈdætə gɹatuitement* maintenant* avec* lə lin dans* lɑ dɪˈskɹɪpʃən.\n",
      " length:151\n",
      " length:149\n",
      "meɪs lə pɹochain* gɹænd outiɫ* d'ia* kju vu devez* connaitɹe* ɛst ˈsteɪbəɫ dɪfˈjuʒən θɹi ˈmidiəm.\n",
      " length:97\n",
      " length:94\n",
      "lɛs poids* də modele* viennent* ˈdɛtɹi publies* ɪɫ waɪ ə quelques* heuɹes*,\n",
      " length:75\n",
      " length:73\n",
      "ɛt sɛst lə modele* də texte* ə ˈɪmɪdʒ lə pləs avance* disponible*.\n",
      " length:66\n",
      " length:65\n",
      "malheuɹeusement*, ɪɫ n'est* disponible* kju suz une* ˈlaɪsəns nɑn kəˌməɹsiˈæɫ, meɪs lə niveau* də qualite* ɛst assez* incɹoyable* ɛt peut* desoɹmais* geneɹeɹ* də manieɹe* fiable* də texte* ə paɹtiɹ* də vɑs ˌɪnˈvaɪts.\n",
      " length:216\n",
      " length:213\n",
      "maintenant*, si vu avez* actuellement* une* pəˈtit ˈæmi ˈiə, dʒi vu ɹecommande* vivement* də lɑ mettɹe* ə niveau* vəɹs ce* ˌnuˈvoʊ modele*.\n",
      " length:139\n",
      " length:136\n",
      "sɛst lɑ peɹsonnalite* ki compte*, meɪs ɛɫ va* paɹaitɹe* assez* moyenne* pɑɹ ɹæˈpɔɹ ə toutes* vɑs amies* səɹ stthɹee*.\n",
      " length:117\n",
      " length:116\n",
      "maintenant*, ˈjuˈɛn autɹe* outiɫ* ki ɛst vɹaiment* utile* ɛst ce* geneɹateuɹ* d'effets* sonoɹes* də ˈilɛvən læbz,\n",
      " length:113\n",
      " length:111\n",
      "lɑ mim entɹepɹise* ki ə concu* mɑ voix*. taʊt ce* kju vu avez* ə fɛɹ ɛst də decɹiɹe* ce* kju vu voulez* ɑnˈtɑndɹə ɛt ɪɫ geneɹeɹa* plusieuɹs* effets* sonoɹes*.\n",
      " length:158\n",
      " length:153\n",
      "voici* ˈjuˈɛn exemple* də du ɹesultats* diffeɹents*. ɛt exemple* du. maintenant* ce* kju dʒi ni vu eɪaɪ pɑz dit*,\n",
      " length:113\n",
      " length:111\n",
      "sɛst qu'un* exemple* etait* ɹiɫ ɛt l'autɹe* etait* geneɹe* pɑɹ l'ia*.\n",
      " length:69\n",
      " length:65\n",
      "dʒi paɹie* kju vu ni pouvez* pɑz sentiɹ* lɑ ˈdɪfəɹəns, meɪs maintenant*,\n",
      " length:72\n",
      " length:72\n",
      "nus devons* ˈpɑɹləɹ də lɑ ˌdʒɛnəɹˈeɪʃən də koʊd. j'ai* attendu* patiemment* kju l'ia* pɹenne* mɑn tɹəˈveɪɫ də pɹogɹammation*,\n",
      " length:125\n",
      " length:123\n",
      "meɪs jusqu'a* ˈpɹɛzənt, j'ai* ete* decu*. cependant*, ɪɫ peut* ˈɑnˌkɔɹ waɪ avoiɹ* də l'espoiɹ*.\n",
      " length:95\n",
      " length:89\n",
      "ɪɫ waɪ ə quelques* semaines*, lɑ ˈstɑɹˌtəp fɹɑnˈsɛz ˈmɪstɹəɫ ə læns ˈjuˈɛn ˌnuˈvoʊ modele* appele* koʊd stɹoʊɫ.\n",
      " length:111\n",
      " length:109\n",
      "sɛst aussi* ˈjuˈɛn modele* ˈoʊpən sɔɹs, meɪs ɪɫ ni peut* pɑz ˈɑnˌkɔɹ etɹe* utilise* ə dɪ fɪnz commeɹciales*,\n",
      " length:108\n",
      " length:106\n",
      "ɛt ɪɫ peɹfoɹme* extɹemement* bin səɹ lɛs ˈbɛntʃˌmɑɹks də codage* pɑɹ ɹæˈpɔɹ ə d'autɹes* modeles* ˈoʊpən sɔɹs.\n",
      " length:109\n",
      " length:107\n",
      "j'ai* joue* avec* səɹ ollama*, ɛt bin qu'iɫ* soit* extɹemement* impɹessionnant* ɛt egalement* tɹes* ɹapide*,\n",
      " length:108\n",
      " length:104\n",
      "ɪɫ ə ˈɑnˌkɔɹ də mæɫ avec* lə vieux* pɹobleme* də centɹeɹ* ˈjuˈɛn div*.\n",
      " length:70\n",
      " length:68\n",
      "maintenant*, quand* ɪɫ s'agit* də l'ia* ecɹivant* də koʊd, ɪɫ waɪ ə du taɪps də peɹsonnes*.\n",
      " length:91\n",
      " length:87\n",
      "ɪɫ waɪ ə lɛs dʒɛnz comme* ˈdɛvɪn ki maximisent* l'ia*, essayant* də fɛɹ ecɹiɹe* pɹɛz də wən ˈhənəɹd% də ˈnoʊtəɹ koʊd pɑɹ l'ia*.\n",
      " length:127\n",
      " length:122\n",
      "ces* peɹsonnes* sont* geneɹalement* jeunes* ɛt naives* selon* lɛs pessimistes* də l'ia* ə l'autɹe* baʊt də ˈspɛktəɹ.\n",
      " length:116\n",
      " length:113\n",
      "ce* sont* geneɹalement* dɪ ˈbuməɹz ki pensent* kju lə koʊd ˈiə ɛst ˈtoʊtəɫ slɑp ɛt n'a* pɑz sɑ pleɪs dans* l'industɹie*.\n",
      " length:120\n",
      " length:117\n",
      "meɪs l'avis* ˈɑptɪməɫ seɪ situe* pɹobablement* quelque* pɑɹt ˈɑntɹə lɛs du.\n",
      " length:75\n",
      " length:72\n",
      "ˈjuˈɛn autɹe* outiɫ* gɹatuit* kju vu pouvez* utiliseɹ* dɪ maintenant* ɛst ˈkəɹsəɹ,\n",
      " length:82\n",
      " length:82\n",
      "ki ɛst ˈjuˈɛn fɔɹk də ˈvəɹsəz koʊd, ɛt l'un* dɪ pɹɛˈmɪɹz editeuɹs* də koʊd ɹeellement* ˈækˌsɪz səɹ l'ia*.\n",
      " length:105\n",
      " length:103\n",
      "oʊ lu də memoɹiseɹ* lɑ syntaxe*, donnez-lui* lə contexte* d'une* beɪs də koʊd existante* u də lɑ ˌdɑkjəmɛnˈteɪʃən,\n",
      " length:114\n",
      " length:112\n",
      "puis* appuyez* səɹ ctɹɫ-k* ɛt ecɹivez* votɹe* koʊd avec* lə langage* natuɹeɫ*.\n",
      " length:78\n",
      " length:75\n",
      "ɛt si vu cɹaignez* qu'iɫ* ecɹive* də koʊd pouɹɹi*, vu pouvez* imposeɹ* ceɹtaines* ɹegles* ɛt mim luɪ fɛɹ effectueɹ* une* ɹɪvˈju də koʊd.\n",
      " length:136\n",
      " length:130\n",
      "sɛst comme* git* həb ˈkoʊpaɪlət suz steɹoides*. l'ia* ˈdʒɛnəɹətɪv ə ˈɑnˌkɔɹ ˈjuˈɛn lɔŋ chemin* ə paɹcouɹiɹ*,\n",
      " length:108\n",
      " length:104\n",
      "meɪs si j'etais* jada* ɛn ce* ˈmoʊmənt, dʒi seɹais* tɹes* pɹeoccupe* pɑɹ lɑ quantite* də pɹogɹes* ˈɹiəˌlaɪzɪz pɑɹ ces* nəɹdz də lɑ saɪəns dɪ donnees* ɛn seulement* lɑ deɹnieɹe* annee*.\n",
      " length:184\n",
      " length:178\n",
      "c'etait* ðə koʊd ɹɪˈpɔɹt. məɹˈsi də ɹegaɹdeɹ*, ɛt dʒi vu veɹɹai* dans* lə pɹochain*.\n",
      " length:84\n",
      " length:81\n"
     ]
    }
   ],
   "source": [
    "source_se = torch.load(f\"{ckpt_base}/en_style_se.pth\").to(device)\n",
    "save_path = f\"{output_dir}/output_whispering.wav\"\n",
    "\n",
    "# Run the base speaker tts\n",
    "# text = \"This audio is generated by OpenVoice.\"\n",
    "\n",
    "text_src = \"resources/texts/en.txt\"\n",
    "\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "text = read_text_from_file(text_src)\n",
    "print(\"Text resource: \", text)\n",
    "\n",
    "src_path = f\"{output_dir}/tmp.wav\"\n",
    "base_speaker_tts.tts(\n",
    "    text, src_path, speaker=\"whispering\", language=\"English\", speed=0.9\n",
    ")\n",
    "\n",
    "# Run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path,\n",
    "    src_se=source_se,\n",
    "    tgt_se=target_se,\n",
    "    output_path=save_path,\n",
    "    message=encode_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcfc70b",
   "metadata": {},
   "source": [
    "**Try with different languages.** OpenVoice can achieve multi-lingual voice cloning by simply replace the base speaker. We provide an example with a Chinese base speaker here and we encourage the readers to try `demo_part2.ipynb` for a detailed demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a71d1387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Taniyow\\miniconda3\\envs\\openvoice\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "d:\\Dev\\ariolas-tech\\rnd\\OpenVoice\\openvoice\\api.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(ckpt_path, map_location=torch.device(self.device))\n",
      "C:\\Users\\Taniyow\\AppData\\Local\\Temp\\ipykernel_19032\\1279953970.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  source_se = torch.load(f\"{ckpt_base}/zh_default_se.pth\").to(device)\n",
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/base_speakers/ZH/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      " > Text splitted to sentences.\n",
      "今天天气真好, 我们一起出去吃饭吧.\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache C:\\Users\\Taniyow\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.570 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tʃ⁼in→tʰjɛn→tʰjɛn→tʃʰi↓ ts`⁼ən→ xɑʊ↓↑,  wo↓↑mən i↓tʃʰi↓↑ ts`ʰu→tʃʰɥ↓ ts`ʰɹ`→fan↓ p⁼a.\n",
      " length:85\n",
      " length:85\n"
     ]
    }
   ],
   "source": [
    "ckpt_base = \"checkpoints/base_speakers/ZH\"\n",
    "base_speaker_tts = BaseSpeakerTTS(f\"{ckpt_base}/config.json\", device=device)\n",
    "base_speaker_tts.load_ckpt(f\"{ckpt_base}/checkpoint.pth\")\n",
    "\n",
    "source_se = torch.load(f\"{ckpt_base}/zh_default_se.pth\").to(device)\n",
    "save_path = f\"{output_dir}/output_chinese.wav\"\n",
    "\n",
    "# Run the base speaker tts\n",
    "text = \"今天天气真好，我们一起出去吃饭吧。\"\n",
    "\n",
    "src_path = f\"{output_dir}/tmp.wav\"\n",
    "base_speaker_tts.tts(text, src_path, speaker=\"default\", language=\"Chinese\", speed=1.0)\n",
    "\n",
    "# Run the tone color converter\n",
    "encode_message = \"@MyShell\"\n",
    "tone_color_converter.convert(\n",
    "    audio_src_path=src_path,\n",
    "    src_se=source_se,\n",
    "    tgt_se=target_se,\n",
    "    output_path=save_path,\n",
    "    message=encode_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e513094",
   "metadata": {},
   "source": [
    "**Tech for good.** For people who will deploy OpenVoice for public usage: We offer you the option to add watermark to avoid potential misuse. Please see the ToneColorConverter class. **MyShell reserves the ability to detect whether an audio is generated by OpenVoice**, no matter whether the watermark is added or not.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d70c38e1c0b038dbdffdaa4f8bfa1f6767c43760905c87a9fbe7800d18c6c35"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
